---
layout: post
title: Load-balanced parallel machine learning benchmarks
description: A new approach using filelock and batchtools
---

```{r Ropts, echo=FALSE, results='hide'}
repo.dir <- normalizePath("..")
post.id <- "2025-05-30-mlr3-filelock"
fig.path <- paste0(file.path(repo.dir, "assets", "img", post.id), "/")
dir.create(fig.path, showWarnings = FALSE, recursive = TRUE)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=fig.path,
  fig.width=10, ## TODO python figures wider? look at prev issue.
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=2)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
options(width=120)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

I recently wrote about [Centralized vs de-centralized
parallelization](https://tdhock.github.io/blog/2025/rush-change-point/).
The goal of this blog is to explore how these new frameworks may be
used for parallelizing machine learning benchmark experiments.

## Introduction to machine learning

Typically when we parallelize machine learning benchmark experiments with mlr3, we use the following pipeline:

* `mlr3::benchmark_grid()` creates a grid of tasks, learners, and resampling iterations.
* `batchtools::makeExperimentRegistry()` creates a registry directory.
* `mlr3batchmark::batchmark()` save the grid into the registry directory.
* `batchtools::submitJobs()` launches the jobs on the cluster.
* `mlr3batchmark::reduceResultsBatchmark()` combines the results into a single file.

This approach is discussed in the following blogs:

* [The importance of hyper-parameter
  tuning](https://tdhock.github.io/blog/2024/hyper-parameter-tuning/)
  explains how to use the auto-tuner.
* [Cross-validation experiments with torch
  learners](https://tdhock.github.io/blog/2024/mlr3torch/) explains
  how to compare linear models in torch with other learners from
  outside torch like `glmnet`.
* [Torch learning with binary
  classification](https://tdhock.github.io/blog/2025/mlr3torch-binary/)
  explains how to implement a custom loss function for binary
  classification in mlr3torch.
* [Comparing neural network architectures using mlr3torch](https://tdhock.github.io/blog/2025/mlr3torch-conv/) explains how to implement different neural network architectures, and make figures to compare their subtrain/validation/test error rates.

While this approach is very useful, it assumes that `submitJobs()` can
efficiently run all the jobs in parallel, which is not always the case:

* Each ML job involves using a single learner on a single task, with a
  single train/test split.
* ML jobs can have very different times, which induces an inefficient use of the scheduler (which requires specifying a single max time/memory for all heterogeneous tasks within a single job/experiment).
* Some clusters like Alliance Canada allow a max of 1000 SLURM jobs
  per user at a time (or 1 SLURM job array with 1000 tasks). This
  includes all entries in the queue, which is problematic for some
  large-scale experiments. For example, our [SOAK
  paper](https://arxiv.org/abs/2410.08643) involved computing 1200+
  jobs on the NAU Monsoon cluster, which did not impose the 1000 SLURM
  job limit.

In this blog, we investigate a method for overcoming this limitation:

* We create a certain number of SLURM jobs using batchtools.
* Each of these jobs looks for work to do in a central CSV table (one row per ML job), and saves the result to disk.
* A column in that table indicates state of job: not run yet, running, done.
* A lock file is used to ensure that only one SLURM job at a time can access and modify that table. 
* When there are no more ML jobs with state "not run yet" then each worker process can exit.

## First install dev mlr3resampling

Some code below requires a development version of mlr3resampling, so we install it first:

```{r}
devtools::install_github("tdhock/mlr3resampling@proj")
```

## Simulate data for ML experiment

This example is taken from `?mlr3resampling::pvalue`:

```{r}
N <- 80
library(data.table)
set.seed(1)
reg.dt <- data.table(
  x=runif(N, -2, 2),
  person=factor(rep(c("Alice","Bob"), each=0.5*N)))
reg.pattern.list <- list(
  easy=function(x, person)x^2,
  impossible=function(x, person)(x^2)*(-1)^as.integer(person))
SOAK <- mlr3resampling::ResamplingSameOtherSizesCV$new()
viz.dt.list <- list()
reg.task.list <- list()
for(pattern in names(reg.pattern.list)){
  f <- reg.pattern.list[[pattern]]
  task.dt <- data.table(reg.dt)[
  , y := f(x,person)+rnorm(N, sd=0.5)
  ][]
  task.obj <- mlr3::TaskRegr$new(
    pattern, task.dt, target="y")
  task.obj$col_roles$feature <- "x"
  task.obj$col_roles$stratum <- "person"
  task.obj$col_roles$subset <- "person"
  reg.task.list[[pattern]] <- task.obj
  viz.dt.list[[pattern]] <- data.table(pattern, task.dt)
}
(viz.dt <- rbindlist(viz.dt.list))
```

The data table above represents simulated regression problem, which is visualized using the code below.

```{r sim-data, fig.height=6}
library(ggplot2)
ggplot()+
  geom_point(aes(
    x, y),
    data=viz.dt)+
  facet_grid(pattern ~ person, labeller=label_both)
```

The figure above shows data for two simulated patterns: 

* easy (top) has same pattern in the two people.
* impossible (bottom) has different patterns in the two people.

## mlr3 benchmark grid

To use mlr3 on these data, we create a benchmark grid with a list of tasks and learners in the code below.

```{r}
reg.learner.list <- list(
  featureless=mlr3::LearnerRegrFeatureless$new())
if(requireNamespace("rpart")){
  reg.learner.list$rpart <- mlr3::LearnerRegrRpart$new()
}
(bench.grid <- mlr3::benchmark_grid(
  reg.task.list,
  reg.learner.list,
  SOAK))
```

The output above indicates that we want to run `same_other_sizes_cv`
resampling on two different tasks, and two different learners.
To run that in sequence, we use the typical mlr3 functions in the code below.

```{r}
lgr::get_logger("mlr3")$set_threshold("warn")
bench.result <- mlr3::benchmark(bench.grid)
bench.score <- mlr3resampling::score(bench.result, mlr3::msr("regr.rmse"))
bench.score[, .(task_id, algorithm, train.subsets, test.fold, test.subset, regr.rmse)]
```

The table of scores above shows the root mean squared error for each
combination of task, algorithm, train subset, test fold, and test
subset. These error values are summarized in the visualization below.

```{r pvalue, fig.height=6}
bench.plist <- mlr3resampling::pvalue(bench.score)
plot(bench.plist)
```

The figure above shows:

* in black, mean plus or minus standard deviation, for each
  combination of task, test subset, algorithm, and train subsets.
* in grey, P-values for differences between same/all and same/other
  (two-sided paired T-test).
  
Exercise for the reader: use the code above to do the computation in
parallel on your laptop, by first declaring `future::plan("multisession")`.
  
## Proposed method for expanding the ML grid

To run the same computations in parallel, we will create a data table
with one row per train/test split, then compute these split results in
parallel.

```{r}
my.grid <- list(
  task_list=reg.task.list,
  learner_list=reg.learner.list,
  resampling=SOAK)
seq.proj.dir <- tempfile()
dir.create(seq.proj.dir)
saveRDS(my.grid, file.path(seq.proj.dir, "grid.rds"))
```

The code above saves a grid of tasks, learners, and a resampling to
the `grid.rds` file, in a project directory called `seq.proj.dir`
(because this first demonstration will be computation in sequence, not
parallel). Next, we create the table with one row per split to
compute,

```{r}
proj_grid_jobs <- function(proj.dir){
  grid.rds <- file.path(proj.dir, "grid.rds")
  proj.grid <- readRDS(grid.rds)
  ml_job_dt_list <- list()
  for(task.name in names(proj.grid$task_list)){
    task.obj <- proj.grid$task_list[[task.name]]
    proj.grid$resampling$instantiate(task.obj)
    for(learner.name in names(proj.grid$learner_list)){
      ml_job_dt_list[[paste(task.name, learner.name)]] <- data.table(
        task.name, learner.name, iteration=seq_len(proj.grid$resampling$iters), status="not started")
    }
  }
  ml_job_dt <- rbindlist(ml_job_dt_list)
  grid_jobs.csv <- file.path(proj.dir, "grid_jobs.csv")
  fwrite(ml_job_dt, grid_jobs.csv)
  ml_job_dt
}
proj_grid_jobs(seq.proj.dir)
```

In the output above, there is one row per train/test split (ML
job). It includes every combination of task, learner, and
cross-validation iteration. After saving these data to disk above, we
can compute one result using the function below.

```{r}
proj_compute <- function(proj.dir){
  library(data.table)
  grid_jobs.csv <- file.path(proj.dir, "grid_jobs.csv")
  grid_jobs.csv.lock <- paste0(grid_jobs.csv, ".lock")
  before.lock <- filelock::lock(grid_jobs.csv.lock)
  grid_jobs_dt <- fread(grid_jobs.csv)
  not.started <- grid_jobs_dt$status == "not started"
  grid_job_i <- NULL
  if(any(not.started)){
    grid_job_i <- which(not.started)[1]
    grid_jobs_dt[grid_job_i, status := "started"]
    fwrite(grid_jobs_dt, grid_jobs.csv)
  }
  filelock::unlock(before.lock)
  if(!is.null(grid_job_i)){
    start.time <- Sys.time()
    grid_job_row <- grid_jobs_dt[grid_job_i]
    grid.rds <- file.path(proj.dir, "grid.rds")
    proj.grid <- readRDS(grid.rds)
    this.task <- proj.grid$task_list[[grid_job_row$task.name]]
    this.learner <- proj.grid$learner_list[[grid_job_row$learner.name]]
    proj.grid$resampling$instantiate(this.task)
    set_rows <- function(train_or_test){
      train_or_test_set <- paste0(train_or_test, "_set")
      set_fun <- proj.grid$resampling[[train_or_test_set]]
      set_fun(grid_job_row$iteration)
    }
    this.learner$train(this.task, set_rows("train"))
    pred <- this.learner$predict(this.task, set_rows("test"))
    result.row <- data.table(
      grid_job_row,
      start.time, end.time=Sys.time(), process=Sys.getpid(),
      learner=list(this.learner),
      pred=list(pred))
    result.rds <- file.path(proj.dir, "grid_jobs", paste0(grid_job_i, ".rds"))
    dir.create(dirname(result.rds), showWarnings = FALSE)
    saveRDS(result.row, result.rds)
    result.row
  }
}
proj_compute(seq.proj.dir)
proj_compute(seq.proj.dir)
```

The output above show the results for two iterations. Each call to
`proj_compute` does the following:

* looks in the project directory and gets a lock on the `grid_jobs.csv` file,
* checks if there are any splits that have not yet started,
* if any have not yet started, we select the next index, `grid_job_i`, to compute next.
* we look in `grid.rds` for the correspond task, learner, and split,
* after calling `train()` and `predict()`, we save the results to a RDS file.
* Importantly, each call to `proj_compute()` only specifies the
  directory to look in, not the particular split to compute. So at
  first, each process does not know what work to do, nor if there is
  any work to do at all! It looks at the file system to determine what
  work to do, if any.
  
Next, we use a while loop in the function below to keep going until there is no more work to do.

```{r}
proj_compute_until_done <- function(proj.dir){
  done <- FALSE
  while(!done){
    result <- proj_compute(proj.dir)
    if(is.null(result))done <- TRUE
  }
}
proj_compute_until_done(seq.proj.dir)
```

Next, we look at the file system and combine the result files.

```{r}
proj_results <- function(proj.dir){
  rbindlist(lapply(Sys.glob(file.path(proj.dir, "grid_jobs", "*.rds")), readRDS))
}
proj_results(seq.proj.dir)
```

Finally, we visualize the results.

```{r times-seq}
norm_times <- function(DT)DT[, let(
  start.seconds=start.time-min(start.time),
  end.seconds=end.time-min(start.time),
  Process=factor(process)
)][]
result.list <- list()
process_viz <- function(name){
  DT <- result.list[[name]]
  ggplot()+
    ggtitle(name)+
    geom_segment(aes(
      start.seconds, Process,
      xend=end.seconds, yend=Process),
      data=DT)+
    geom_point(aes(
      start.seconds, Process),
      shape=1,
      data=DT)+
    scale_x_continuous("Seconds from start of computation")
}
result.list$laptop_sequential <- norm_times(proj_results(seq.proj.dir))
process_viz("laptop_sequential")
```

The output above shows each split as a dot (for start time) and line
segment (which goes until the end time). It is clear that there was
one process used to compute all of the results.

## Laptop in parallel using batchtools

To compute in parallel, we can use `batchtools`.
We first setup a new project with a registry:

```{r}
multi.proj.dir <- tempfile()
dir.create(multi.proj.dir)
saveRDS(my.grid, file.path(multi.proj.dir, "grid.rds"))
multi.job.dt <- proj_grid_jobs(multi.proj.dir)
reg.dir <- file.path(multi.proj.dir, "registry")
reg <- batchtools::makeRegistry(reg.dir)
```

Note in the output above that we are using Multicore cluster
functions, because we have the following code in the user-specific
config file:

```{r}
cat(readLines("~/.batchtools.conf.R"), sep="\n")
```

The code below launches the parallel computation in 2 worker processes.

```{r times-multi}
n.workers <- 2
bm.jobs <- batchtools::batchMap(
  function(i)proj_compute_until_done(multi.proj.dir),
  seq(1, n.workers))
batchtools::submitJobs(bm.jobs)
batchtools::waitForJobs()
```

The code below visualizes the result.

```{r}
result.list$laptop_multicore <- norm_times(proj_results(multi.proj.dir))
process_viz("laptop_multicore")
```

The output above shows that there are two processes working in
parallel on the computation.  After each process finishes a given
split, it looks for a new split to work on, and starts right away (it
does not have to wait for the other process to finish any work).

## Using package functions

Based on the ideas in the functions above, I wrote some analogous functions in my `mlr3resampling` R package. 
These functions allow runnning these kind of parallel ML experiments either locally, or on a SLURM compute cluster.
Below, we show how the local multi-core computation works.

```{r times-multi-pkg}
pkg.proj.dir <- '2025-05-30-mlr3-filelock'
unlink(pkg.proj.dir, recursive=TRUE)
mlr3resampling::proj_grid(
  pkg.proj.dir,
  reg.task.list,
  reg.learner.list,
  SOAK,
  order_jobs = function(DT)DT[,order(test.subset)],
  score_args=mlr3::msrs(c("regr.rmse", "regr.mae")))
```

Above we create a new project based on the same grid as in previous sections.
Other arguments include `order_jobs`, which returns an integer vector that specifies the priority/order of execution of the different ML experiments. This could be useful for the situation where you run out of time on SLURM, in which case only the entries at the top of the table will have results on the file system. In that case, you can also use the `order_jobs` argument to resume work from where you left off (only provide the indices of arguments that have not yet completed).

```{r}
mlr3resampling::proj_submit(pkg.proj.dir)
batchtools::waitForJobs()

result.list$laptop_pkg <- norm_times(fread(file.path(pkg.proj.dir,"results.csv")))
process_viz("laptop_pkg")
```

```{r times-mammouth}
result.list$mammouth_slurm <- norm_times(fread("2025-05-30-mlr3-filelock-mammouth/results.csv"))
process_viz("mammouth_slurm")
```

```{r times-beluga}
result.list$beluga_slurm <- norm_times(fread("2025-05-30-mlr3-filelock-beluga/results.csv"))
process_viz("beluga_slurm")
```

## Compare them all

```{r times-compare, fig.height=6}
common.names <- Reduce(intersect, sapply(result.list, names))
compare_results_list <- list()
for(computer in names(result.list)){
  compare_results_list[[computer]] <- data.table(
    computer,
    result.list[[computer]][, common.names, with=FALSE])
}
(compare_results <- rbindlist(compare_results_list))
ggplot()+
  geom_segment(aes(
    start.seconds, Process,
    xend=end.seconds, yend=Process),
    data=compare_results)+
  geom_point(aes(
    start.seconds, Process),
    shape=1,
    data=compare_results)+
  scale_x_continuous("Seconds from start of computation")+
  facet_grid(computer ~ ., scales="free")
```

## Conclusions

TODO

## Session info

```{r}
sessionInfo()
```
