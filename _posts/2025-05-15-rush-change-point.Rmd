---
layout: post
title: Centralized vs de-centralized parallelization
description: Exploring rush
---

```{r Ropts, echo=FALSE, results='hide'}
repo.dir <- normalizePath("..")
post.id <- "2025-05-15-rush-change-point"
fig.path <- paste0(file.path(repo.dir, "assets", "img", post.id), "/")
dir.create(fig.path, showWarnings = FALSE, recursive = TRUE)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=fig.path,
  fig.width=10, ## TODO python figures wider? look at prev issue.
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=6)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
options(width=120)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

I recently wrote about [New parallel computing frameworks in
R](https://tdhock.github.io/blog/2025/rush/), including the
[rush](https://github.com/mlr-org/rush) package by Marc Becker, who I
met in a visit to Bernd Bischl's lab in Munich this week.  The goal of
this blog is to explore different ways rush can be used for parallel
computation of change-point models.

## Genomic data

In the PeakSegDisk package, we have the Mono27ac data set, which
represents part of a genomic profile, with several peaks that
represent active regions.

```{r}
data(Mono27ac, package="PeakSegDisk")
library(data.table)
Mono27ac$coverage
```

These data can be visualized via the code below.

```{r data}
library(ggplot2)
ggplot()+
  theme_bw()+
  geom_step(aes(
    chromStart/1e3, count),
    color="grey50",
    data=Mono27ac$coverage)
```

## Computing a peak model

We may like to compute a sequence of change-point models for these
data.  In the PeakSegDisk package, you can use the
`sequentialSearch_dir` function to compute a model with a given number
of peaks. You first must save the data to disk, as in the code below.

```{r}
data.dir <- file.path(
  tempfile(),
  "H3K27ac-H3K4me3_TDHAM_BP",
  "samples",
  "Mono1_H3K27ac",
  "S001YW_NCMLS",
  "problems",
  "chr11-60000-580000")
dir.create(data.dir, recursive=TRUE, showWarnings=FALSE)
write.table(
  Mono27ac$coverage, file.path(data.dir, "coverage.bedGraph"),
  col.names=FALSE, row.names=FALSE, quote=FALSE, sep="\t")
```

After that, you can run the sequential search algorithm, in parallel
if you declare a future plan as in the code below.

```{r}
future::plan("multisession")
fit10 <- PeakSegDisk::sequentialSearch_dir(data.dir, 10L, verbose=1)
```

The code uses a penalized solver, which means a non-negative penalty
value must be specified as the model complexity hyper-parameter, and
we do not know in advance how many change-points (and peaks) that will
give. The sequential search algorithm evaluates a bunch of different
penalty values until it finds the desired number of peaks (10 in this
case). This model is shown below,

```{r peaks-10}
plot(fit10)+
  geom_step(aes(
    chromStart, count),
    color="grey50",
    data=Mono27ac$coverage)
```

The plot above shows that the model change-points are a reasonable fit
to the data, but we may want to examine other model sizes (say from 5
to 15 peaks). 

```{r}
target.min.peaks <- 5
target.max.peaks <- 15
```

To do that, we could implement a parallel computation of
the penalty values.  `Next` output above shows the penalties that were
evaluated in the search. We see that the first two penalties are `0`
and `Inf`, which give the largest/smallest change-point models (3199
and 0 peaks in this case). Those are the only two that are evaluated
in parallel. Next, we show how we could evaluate several penalties in
parallel.

## Computing a range of models

Computing a range of change-point model sizes can be implemented using
the CROCS algorithm, described in [our BMC Bioinformatics 2021 paper,
with Arnaud Liehrmann and Guillem
Rigaill](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04221-5).
To do that, we first compute the most extreme models.

```{r}
pen.vec <- c(0,Inf)
loss.dt.list <- list()
for(pen in pen.vec){
  fit <- PeakSegDisk::PeakSegFPOP_dir(data.dir, pen)
  loss.dt.list[[paste(pen)]] <- fit$loss[, .(penalty, peaks, total.loss)]
}
(loss.dt <- rbindlist(loss.dt.list))
```

The table above shows the number of peaks and Poisson loss values for
the two models. The optimization objective is the cost plus the
penalty times the number of peaks. Therefore, the number of peaks
selected for a given penalty is the argmin of a finite number of
linear functions. Each linear function has slope equal to number of
peaks, and intercept equal to total Poisson loss.  We can compute an
estimate of a model selection function, which maps penalty/lambda
values to model sizes (in peaks) via the code below.

```{r}
get_selection <- function(loss){
  selection.df <- penaltyLearning::modelSelection(loss, "total.loss", "peaks")
  with(selection.df, data.table(
    total.loss,
    min.lambda,
    penalty,
    max.lambda,
    peaks_after=c(peaks[-1],NA),
    peaks
  ))[
  , helpful := peaks_after<target.max.peaks & peaks>target.min.peaks &
      peaks_after+1 < peaks & !max.lambda %in% names(loss.dt.list)
  ][]
}
(selection.dt <- get_selection(loss.dt))
```

The table above has columns for `min.lambda` and `max.lambda`, which
indicate a range of penalties for which we have evidence that `peaks`
may be selected. We can visualize these functions via the plot below,

```{r model-sel}
get_gg <- function(){
  dot.dt <- rbind(
    data.table(loss.dt[, .(
      penalty, peaks, total.loss, helpful=FALSE)], dot="evaluated"),
    data.table(selection.dt[-.N, .(
      penalty=max.lambda, peaks, total.loss, helpful)], dot="candidate"))
  ggplot()+
    geom_abline(aes(
      slope=peaks,
      intercept=total.loss),
      data=loss.dt)+
    geom_point(aes(
      penalty,
      total.loss+ifelse(peaks==0, 0, penalty*peaks),
      color=helpful),
      size=5,
      data=dot.dt)+
    geom_point(aes(
      penalty,
      total.loss+ifelse(peaks==0, 0, penalty*peaks),
      color=helpful,
      fill=dot),
      shape=21,
      size=4,
      data=dot.dt)+
    geom_label(aes(
      penalty,
      total.loss+ifelse(peaks==0, 0, penalty*peaks),
      hjust=ifelse(penalty==Inf, 1, 0.5),
      label=peaks),
      vjust=1.5,
      alpha=0.5,
      data=dot.dt[dot=="evaluated"])+
    scale_fill_manual(values=c(evaluated="white", candidate="black"))+
    scale_color_manual(values=c("TRUE"="red", "FALSE"="black"))+
    scale_y_continuous("Cost = Loss + penalty * peaks")
}
get_gg()
pen <- selection.dt$max.lambda[1]
```

We see in the plot above that there is an intersection of these
functions at penalty=`r pen`. Since these functions are for
3199 and 0 peaks, and our target numbers of peaks is between (5 to
15), we are guaranteed to make progress if we compute a new penalty at
the intersection point. We do that in the code below,

```{r}
fit <- PeakSegDisk::PeakSegFPOP_dir(data.dir, pen)
loss.dt.list[[paste(pen)]] <- fit$loss[, .(penalty, peaks, total.loss)]
(loss.dt <- rbindlist(loss.dt.list))
```

We see in the new loss table that there is an intermediate number of
peaks, `r loss.dt[.N, peaks]`.  This results in two intersection point
candidates (the first two rows of the table below).

```{r model-sel-3}
(selection.dt <- get_selection(loss.dt))
get_gg()
```

We see in the figure above that only one of the two candidates is helpful toward our goal of computing models between 5 and 15 peaks.
We can evaluate the penalty at that candidate in order to make progress.
Let's keep going until we have more than one interesting/helpful candidate.

```{r}
while(sum(selection.dt$helpful)==1){
  pen <- selection.dt[helpful==TRUE, max.lambda]
  fit <- PeakSegDisk::PeakSegFPOP_dir(data.dir, pen)
  print(loss.dt.list[[paste(pen)]] <- fit$loss[, .(penalty, peaks, total.loss)])
  loss.dt <- rbindlist(loss.dt.list)
  selection.dt <- get_selection(loss.dt)
}
selection.dt
```

The output above shows that there are now two helpful candidates. The
code below visualizes them.

```{r model-sel-6}
get_gg()
```

Above we see that there are 5 candidate penalties, but only 2 helpful
penalties that would make progress toward computing all
models between 5 and 15 peaks.
At this point we may think of how to parallelize.

## Centralized launching, future.apply

One way to do it is by first computing the candidate penalties in a
central process, then sending those penalties to workers for
computation. That is implemented in the code below.

```{r pit-1}
pen.list <- list()
seq_it <- function(){
  pen.vec <- selection.dt[helpful==TRUE, max.lambda]
  pen.list[[length(pen.list)+1]] <<- pen.vec
  loss.dt.list[paste(pen.vec)] <<- future.apply::future_lapply(pen.vec, function(pen){
    fit <- PeakSegDisk::PeakSegFPOP_dir(data.dir, pen)
    fit$loss[, .(penalty, peaks, total.loss)]
  })
  loss.dt <<- rbindlist(loss.dt.list)
  selection.dt <<- get_selection(loss.dt)
  get_gg()
}
seq_it()
```

After the first parallel iteration above, we see four helpful candidates.

```{r pit-2}
seq_it()
```

Let's keep going.

```{r pit-3}
while(any(selection.dt$helpful)){
  gg <- seq_it()
}
gg
selection.dt
```

We see in the table above that there are 16 iterations total.

### Coding a function

Overall the algorithm which uses a central launcher to determine
candidates can be implemented via the code below.

```{r}
central_launch <- function(target.min.peaks, target.max.peaks){
  data.dir <- file.path(
    tempfile(),
    "H3K27ac-H3K4me3_TDHAM_BP",
    "samples",
    "Mono1_H3K27ac",
    "S001YW_NCMLS",
    "problems",
    "chr11-60000-580000")
  dir.create(data.dir, recursive=TRUE, showWarnings=FALSE)
  write.table(
    Mono27ac$coverage, file.path(data.dir, "coverage.bedGraph"),
    col.names=FALSE, row.names=FALSE, quote=FALSE, sep="\t")
  pen.vec <- c(0,Inf)
  loss.dt.list <- list()
  cand.dt.list <- list()
  iteration <- 1L
  while(length(pen.vec)){
    loss.dt.list[paste(pen.vec)] <- future.apply::future_lapply(
      pen.vec, function(pen){
        start.time <- Sys.time()
        fit <- PeakSegDisk::PeakSegFPOP_dir(data.dir, pen)
        fit$loss[, .(
          penalty, peaks, total.loss, iteration,
          process=factor(Sys.getpid()), start.time, end.time=Sys.time())]
      }
    )
    start.time <- Sys.time()
    loss.dt <- rbindlist(loss.dt.list)
    selection.df <- penaltyLearning::modelSelection(loss.dt, "total.loss", "peaks")
    selection.dt <- with(selection.df, data.table(
      total.loss,
      min.lambda,
      penalty,
      max.lambda,
      peaks_after=c(peaks[-1],NA),
      peaks
    ))[
    , helpful := peaks_after<target.max.peaks & peaks>target.min.peaks &
        peaks_after+1 < peaks & !max.lambda %in% names(loss.dt.list)
    ][]
    pen.vec <- selection.dt[helpful==TRUE, max.lambda]
    cand.dt.list[[length(cand.dt.list)+1]] <- data.table(
      iteration,
      process=factor("central"), start.time, end.time=Sys.time())
    iteration <- iteration+1L
  }
  list(loss=loss.dt, candidates=rbindlist(cand.dt.list))
}
(loss_5_15 <- central_launch(5,15))
```

The result is a list of two tables:

* `loss` has one row per penalty value computed.
* `candidates` has one row per computation of new candidates.

```{r central-5-15}
viz_workers <- function(L){
  min.time <- min(L$loss$start.time)
  for(N in names(L)){
    L[[N]] <- data.table(L[[N]], computation=N)
    for(pos in c("start", "end")){
      time.vec <- L[[N]][[paste0(pos,".time")]]
      set(L[[N]], j=paste0(pos,".seconds"), value=time.vec-min.time)
    }
  }
  ##L$cand$process <- L$loss[, .(count=.N), by=process][order(-count)][1, process]
  biggest.diff <- L$loss[,as.numeric(diff(c(min(start.seconds),max(end.seconds))))]
  n.proc <- length(unique(L$loss$process))
  best.seconds <- n.proc*biggest.diff
  total.seconds <- L$loss[,sum(end.seconds-start.seconds)]
  efficiency <- total.seconds/best.seconds
  seg.dt <- rbind(L$cand,L$loss[,names(L$cand),with=FALSE])
  text.dt <- L$loss[,.(
    mid.seconds=as.POSIXct((as.numeric(min(start.seconds))+as.numeric(max(end.seconds)))/2),
    label=paste(peaks,collapse=",")
  ),by=.(process,iteration)]
  it.dt <- seg.dt[, .(min.seconds=min(start.seconds), max.seconds=max(end.seconds)), by=iteration]
  ggplot()+
    theme_bw()+
    ggtitle(sprintf("Worker efficiency = %.1f%%", 100*total.seconds/best.seconds))+
    geom_rect(aes(
      xmin=min.seconds, xmax=max.seconds,
      ymin=-Inf, ymax=Inf),
      fill="grey",
      alpha=0.5,
      data=it.dt)+
    geom_text(aes(
      x=(min.seconds+max.seconds)/2, Inf,
      hjust=ifelse(iteration==1, 1, 0.5),
      label=paste0(ifelse(iteration==1, "it=", ""), iteration)),
      data=it.dt,
      vjust=1)+
    directlabels::geom_dl(aes(
      start.seconds, process,
      label.group=paste(iteration, process),
      label=peaks),
      data=L$loss,
      method=polygon.mine("bottom", offset.cm=0.2, custom.colors=list(box.color="white")))+
    geom_segment(aes(
      start.seconds, process,
      color=computation,
      xend=end.seconds, yend=process),
      data=seg.dt)+
    geom_point(aes(
      start.seconds, process,
      color=computation),
      shape=1,
      data=seg.dt)+
    ## geom_text(aes(
    ##   mid.seconds, process, label=label),
    ##   vjust=1.5,
    ##   data=text.dt)+
    scale_color_manual(values=c(
      loss="black",
      candidates="red"))+
    scale_y_discrete("process")+
    scale_x_continuous("time (seconds)")
}
polygon.mine <- function
### Make a Positioning Method that places non-overlapping speech
### polygons at the first or last points.
(top.bottom.left.right,
### Character string indicating what side of the plot to label.
  offset.cm=0.1,
### Offset from the polygon to the most extreme data point.
  padding.cm=0.05,
### Padding inside the polygon.
  custom.colors=NULL
### Positioning method applied just before draw.polygons, can set
### box.color and text.color for custom colors.
){
  if(is.null(custom.colors)){
    custom.colors <- directlabels::gapply.fun({
      rgb.mat <- col2rgb(d[["colour"]])
      d$text.color <- with(data.frame(t(rgb.mat)), {
        gray <- 0.3*red + 0.59*green + 0.11*blue
        ifelse(gray/255 < 0.5, "white", "black")
      })
      d
    })
  }
  opposite.side <- c(
    left="right",
    right="left",
    top="bottom",
    bottom="top")[[top.bottom.left.right]]
  direction <- if(
    top.bottom.left.right %in% c("bottom", "left")
  ) -1 else 1
  min.or.max <- if(
    top.bottom.left.right %in% c("top", "right")
  ) max else min
  if(top.bottom.left.right %in% c("left", "right")){
    min.or.max.xy <- "x"
    qp.target <- "y"
    qp.max <- "top"
    qp.min <- "bottom"
    padding.h.factor <- 2
    padding.w.factor <- 1
    limits.fun <- ylimits
    reduce.method <- "reduce.cex.lr"
  }else{
    min.or.max.xy <- "y"
    qp.target <- "x"
    qp.max <- "right"
    qp.min <- "left"
    padding.h.factor <- 1
    padding.w.factor <- 2
    limits.fun <- directlabels::xlimits
    reduce.method <- "reduce.cex.tb"
  }
  list(
    hjust=0.5, vjust=1,
    function(d,...){
      ## set the end of the speech polygon to the original data point.
      for(xy in c("x", "y")){
        extra.coord <- sprintf(# e.g. left.x
          "%s.%s", opposite.side, xy)
        d[[extra.coord]] <- d[[xy]]
      }
      ## offset positions but do NOT set the speech polygon position
      ## to the min or max.
      d[[min.or.max.xy]] <- d[[min.or.max.xy]] + offset.cm*direction
      d
    },
    "calc.boxes",
    reduce.method,
    function(d, ...){
      d$h <- d$h + padding.cm * padding.h.factor
      d$w <- d$w + padding.cm * padding.w.factor
      d
    },
    "calc.borders",
    function(d,...){
      do.call(rbind, lapply(split(d, d$y), function(d){
        directlabels::apply.method(directlabels::qp.labels(
          qp.target,
          qp.min,
          qp.max,
          directlabels::make.tiebreaker(min.or.max.xy, qp.target),
          limits.fun), d)
      }))
    },
    "calc.borders",
    custom.colors,
    "draw.polygons")
}
viz_workers(loss_5_15)
```

The figure above shows how the computation proceeds over time (X axis)
in the different parallel processes (Y axis). Iterations are shown in
grey rectangles, with iteration numbers at the top of the plot. In
each iteration, There are at most four parallel workers (black) at any
given time. We see that before starting a new iteration, all workers
must complete, and wait for the central process to compute new
candidates. The worker efficiency reported at the top of the plot is
the amount of time taken computing models (black segments), divided by
the max time that could be taken by that number of workers (if black
line segments were from start to end of the plot for each
worker). There are two sources of inefficiency:

* The overhead for starting a new future job is about 0.2 seconds, and
  this overhead could be reduced by using mirai package instead. But
  for long running computations (big data or complex models), this
  overhead is not the important bottleneck.
* Because of the centralized method for computing new candidates, the
  first process that finishes in an iteration must wait for the last
  process, before it starts working again. This can be fixed by using
  rush package instead (as we show in the next section).

To see these patterns more clearly, below we run the central launch
algorithm with a larger number of models:

```{r central-1-100}
(loss_1_100 <- central_launch(1,100))
viz_workers(loss_1_100)
```

The figure above shows more iterations and more processes. In some
iterations (9-11), the number of candidates is greater than 14, which
is the number of CPUs on my machine (and the max number of future
workers). In those iterations, we see calculation of either 1 or 2
models in each worker process.

## De-centralized candidate computation, rush

TODO
