---
layout: post
title: Overhead of auto-grad in torch
description: Comparison with explicit gradients
---

```{r Ropts, echo=FALSE, results='hide'}
repo.dir <- normalizePath("..")
post.id <- "2024-10-28-auto-grad-overhead"
fig.path <- paste0(file.path(repo.dir, "assets", "img", post.id), "/")
dir.create(fig.path, showWarnings = FALSE, recursive = TRUE)
knitr::opts_chunk$set(
  dpi=200,
  fig.path=fig.path,
  fig.width=7.5, ## TODO python figures wider? look at prev issue.
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=3)
options(width=120)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

The goal of this post is to show how to use R torch to compute AUM
(Area Under Min of False Positive and False Negative rates, our newly
Proposed surrogate loss for ROC curve optimization).

* Paper: [Optimizing ROC Curves with a Sort-Based Surrogate Loss for Binary Classification and Changepoint Detection](https://jmlr.org/papers/v24/21-0751.html).
* Slides [PDF](https://github.com/tdhock/max-generalized-auc/blob/master/HOCKING-slides-toronto.pdf).

This post is similar to what
we did in [a previous blog post using
python](https://tdhock.github.io/blog/2024/torch-roc-aum/). This blog
uses R instead of python, in order to compare the auto-grad from torch
with the "explicit gradient" computed the `aum` R package.

## Introduction to binary classification and ROC curves

In supervised binary classification, our goal is to learn a function
`f` using training inputs/features `x`, and outputs/labels `y`, such
that `f(x)=y` (on new/test data). To illustrate, the code below
defines a data set with four samples:

```{r}
four_labels_vec <- c(-1,-1,1,1)
four_pred_vec <- c(2.0, -3.5, -1.0, 1.5)
```

The first to samples are negative, and the second two are
positive. The predicted scores are real numbers, which we threshold at
zero to determine a predicted class. The ROC curve shows how the
prediction error rates change, as we add constants from negative to
positive infinity to the predicted scores. To compute the ROC curve using torch, we can use the code below.

```{r}
ROC_curve <- function(pred_tensor, label_tensor){
  is_positive = label_tensor == 1
  is_negative = label_tensor != 1
  fn_diff = torch::torch_where(is_positive, -1, 0)
  fp_diff = torch::torch_where(is_positive, 0, 1)
  thresh_tensor = -pred_tensor$flatten()
  sorted_indices = torch::torch_argsort(thresh_tensor)
  fp_denom = torch::torch_sum(is_negative) #or 1 for AUM based on count instead of rate
  fn_denom = torch::torch_sum(is_positive) #or 1 for AUM based on count instead of rate
  sorted_fp_cum = fp_diff[sorted_indices]$cumsum(dim=1)/fp_denom
  sorted_fn_cum = -fn_diff[sorted_indices]$flip(1)$cumsum(dim=1)$flip(1)/fn_denom
  sorted_thresh = thresh_tensor[sorted_indices]
  sorted_is_diff = sorted_thresh$diff() != 0
  sorted_fp_end = torch::torch_cat(c(sorted_is_diff, torch::torch_tensor(TRUE)))
  sorted_fn_end = torch::torch_cat(c(torch::torch_tensor(TRUE), sorted_is_diff))
  uniq_thresh = sorted_thresh[sorted_fp_end]
  uniq_fp_after = sorted_fp_cum[sorted_fp_end]
  uniq_fn_before = sorted_fn_cum[sorted_fn_end]
  FPR = torch::torch_cat(c(torch::torch_tensor(0.0), uniq_fp_after))
  FNR = torch::torch_cat(c(uniq_fn_before, torch::torch_tensor(0.0)))
  list(
    FPR=FPR,
    FNR=FNR,
    TPR=1 - FNR,
    "min(FPR,FNR)"=torch::torch_minimum(FPR, FNR),
    min_constant=torch::torch_cat(c(torch::torch_tensor(-Inf), uniq_thresh)),
    max_constant=torch::torch_cat(c(uniq_thresh, torch::torch_tensor(Inf))))
}
four_labels <- torch::torch_tensor(four_labels_vec)
four_pred <- torch::torch_tensor(four_pred_vec)
list.of.tensors <- ROC_curve(four_pred, four_labels)
data.frame(lapply(list.of.tensors, torch::as_array))
```

The table above also has one row for each point on the ROC curve, and the following columns:

* `FPR` is the False Positive Rate (X axis of ROC curve plot),
* `TPR` is the True Positive Rate (Y axis of ROC curve plot),
* `FNR=1-TPR` is the False Negative Rate,
* `min(FPR,FNR)` is the minimum of `FPR` and `FNR`,
* and `min_constant`, `max_constant` give the range of constants which result in the corresponding error values. For example, the second row means that adding any constant between -2 and -1.5 results in predicted classes that give FPR=0.5 and TPR=0.

How do we interpret the ROC curve? An ideal ROC curve would

* start at the bottom left (FPR=TPR=0, every sample predicted negative), 
* and then go straight to the upper left (FPR=0,TPR=1, every sample
  predicted correctly),
* and then go straight to the upper right (FPR=TPR=1, every sample
  predicted positive),
* so it would have an Area Under the Curve of 1.

So when we do ROC analysis, we can look at the curves, to see how close they get to the upper left, or we can just compute the Area Under the Curve (larger is better). To compute the Area Under the Curve, we use the trapezoidal area formula, which amounts to summing the rectangle and triangle under each segment of the curve, as in the code below.

```{r}
ROC_AUC <- function(pred_tensor, label_tensor){
  roc = ROC_curve(pred_tensor, label_tensor)
  FPR_diff = roc$FPR[2:N]-roc$FPR[1:-2]
  TPR_sum = roc$TPR[2:N]+roc$TPR[1:-2]
  torch::torch_sum(FPR_diff*TPR_sum/2.0)
}
ROC_AUC(four_pred, four_labels)
```

The ROC AUC value is 0.5, indicating that `four_pred` is not a very
good vector of predictions with respect to `four_labels`.

## Proposed AUM loss for ROC optimization

Recently [in JMLR23](https://jmlr.org/papers/v24/21-0751.html)
we proposed a new loss function called the AUM, Area Under Min of
False Positive and False Negative rates. We showed that is can be
interpreted as a L1 relaxation of the sum of min of False Positive and
False Negative rates, over all points on the ROC curve. We
additionally showed that AUM is piecewise linear, and differentiable
almost everywhere, so can be used in gradient descent learning
algorithms. Finally, we showed that minimizing AUM encourages points
on the ROC curve to move toward the upper left, thereby encouraging
large AUC. Computation of the AUM loss requires first
computing ROC curves (same as above), as in the code below.

```{r}
Proposed_AUM <- function(pred_tensor, label_tensor){
  roc = ROC_curve(pred_tensor, label_tensor)
  min_FPR_FNR = roc[["min(FPR,FNR)"]][2:-2]
  constant_diff = roc$min_constant[2:N]$diff()
  torch::torch_sum(min_FPR_FNR * constant_diff)
}
```

The implementation above uses the `ROC_curve` sub-routine, to
emphasize the similarity with the AUC computation. The `Proposed_AUM`
function is a differentiable surrogate loss that can be used to
compute gradients. To do that, we first tell torch that the vector of
predicted values requires a gradient, then we call `backward()` on the
result from `Proposed_AUM`, which assigns the `grad` attribute of the
predictions, as can be seen below:

```{r}
four_pred$requires_grad <- TRUE
(four_aum <- Proposed_AUM(four_pred, four_labels))
four_pred$grad
four_aum$backward()
four_pred$grad
```

We can compare the result from auto-grad above, to the result from the
R `aum` package, which implements a function that gives "explicit
gradients," actually [a dedicated `aum_sort` C++
function](https://github.com/tdhock/aum/blob/main/src/aum_sort.cpp)
that computes a matrix of directional derivatives (since AUM has
non-differentiable points). We see that the gradient vector above
(from `torch` auto-grad) is consistent with the directional derivative
matrix below (from R package `aum`):

```{r}
four_labels_diff_dt <- aum::aum_diffs_binary(four_labels_vec, denominator = "rate")
aum::aum(four_labels_diff_dt, four_pred_vec)
```

The AUM loss and its gradient can be visualized using the setup below.

* We assume there are two samples: one positive label, and one negative label.
* We plot the AUM loss and its gradient (with respect to the two
  predicted scores) for a grid different values of `f(x1)` (predicted
  score for positive example), while keeping constant `f(x0)`
  (predicted score for negative example).
* We represent these in the plot below on an X axis called "Difference
  between predicted scores" because AUM only depends on the
  difference/rank of predicted scores (not absolute values).

```{r autoVsExplicitSubGradients}
label_vec = c(0, 1)
pred_diff_vec = seq(-3, 3, by=0.5)
aum_grad_dt_list = list()
library(data.table)
for(pred_diff in pred_diff_vec){
  pred_vec = c(0, pred_diff)
  pred_tensor = torch::torch_tensor(pred_vec)
  pred_tensor$requires_grad = TRUE
  label_tensor = torch::torch_tensor(label_vec)
  loss = Proposed_AUM(pred_tensor, label_tensor)
  loss$backward()
  g_vec = torch::as_array(pred_tensor$grad)
  diff_dt <- aum::aum_diffs_binary(label_vec, denominator = "rate")
  aum_info <- aum::aum(diff_dt, pred_vec)
  grad_list <- list(
    explicit=aum_info$derivative_mat,
    auto=cbind(g_vec, g_vec))
  for(method in names(grad_list)){
    grad_mat <- grad_list[[method]]
    colnames(grad_mat) <- c("min","max")
    aum_grad_dt_list[[paste(pred_diff, method)]] <- data.table(
      label=label_vec, pred_diff, method, grad_mat)
  }
}
(aum_grad_dt <- rbindlist(aum_grad_dt_list)[
, mean := (min+max)/2
][])
aum_grad_points <- melt(
  aum_grad_dt,
  measure.vars=c("min","max","mean"),
  variable.name="direction")
library(ggplot2)
ggplot()+
  scale_fill_manual(
    values=c(
      min="white",
      mean="red",
      max="black"))+
  scale_size_manual(
    values=c(
      min=6,
      mean=2,
      max=4))+
  geom_point(aes(
    pred_diff, value, fill=direction, size=direction),
    shape=21,
    data=aum_grad_points)+
  geom_segment(aes(
    pred_diff, min,
    xend=pred_diff, yend=max),
    data=aum_grad_dt)+
  scale_x_continuous(
    "Difference between predicted scores, f(x1)-f(x0)")+
  facet_grid(method ~ label, labeller=label_both)
```

The figure above shows the [sub-differential](https://en.wikipedia.org/wiki/Subderivative) as a function of difference between predicted scores.

* Label: 0 (left) shows the range of sub-gradients with respect to the predicted score for the negative example, 
* Label: 1 (right) shows the range of sub-gradients with respect to the predicted score for the positive example, 
* For method: auto (top), the torch auto-grad returns a sub-gradient (min=max).
* For method: explicit, the `derivative_mat` returned from `aum::aum` is shown. When the difference between predicted scores is 0, there is a non-differentiable point, which can be seen as a range of sub-gradients, from `min` to `max`. For gradient descent, we can use `mean` as the "gradient."
* These sub-gradients mean that when the difference between predicted scores is negative, the AUM can be decreased by increasing the predicted score for the positive example, or by decreasing the predicted score for the negative example.

## Time comparison

In this section, we compare the computation time of the two methods
for computing gradients.

```{r atimeGrad}
a_res <- atime::atime(
  N=as.integer(10^seq(2,6,by=0.2)),
  setup={
    set.seed(1)
    pred_vec = rnorm(N)
    label_vec = rep(0:1, l=N)
  },
  auto={
    pred_tensor = torch::torch_tensor(pred_vec, "double")
    pred_tensor$requires_grad = TRUE
    label_tensor = torch::torch_tensor(label_vec)
    loss = Proposed_AUM(pred_tensor, label_tensor)
    loss$backward()
    torch::as_array(pred_tensor$grad)
  },
  explicit={
    diff_dt <- aum::aum_diffs_binary(label_vec, denominator = "rate")
    aum.info <- aum::aum(diff_dt, pred_vec)
    rowMeans(aum.info$derivative_mat)
  },
  result = TRUE,
  seconds.limit=1
)
plot(a_res)
```

Above we see that `auto` has about 10x more computation time overhead
than `explicit` (for small N, less than 1e3). But we see that for
large N (at least 1e5), they have asymptotically the same computation
time. This result implies that auto-grad in torch is actually just as
fast as the explicit gradient computation using the C++ code from R
package `aum`. We can also see constant factor differences in memory
usage, because `atime` only measures R memory (not torch memory, so
that usage is under-estimated).


Note that we used `torch::torch_tensor(pred_vec, "double")` above, for
consistency with R double precision numeric vectors (torch default is
single precision, which can lead to numerical differences for large N,
exercise for the reader, use the code below to explore those
differences).
Below we verify that the results are the same.

```{r}
res.long <- a_res$measurements[, {
  grad <- result[[1]]
  data.table(grad, i=seq_along(grad))
}, by=.(N,expr.name)]
(res.wide <- dcast(
  res.long, N + i ~ expr.name, value.var="grad"
)[
, diff := auto-explicit
][])
res.wide[is.na(diff)]
dcast(res.wide, N ~ ., list(mean, median, max), value.var="diff")
```

The result above shows that there are very little differences between the gradients in the two methods.

Below we estimate the asymptotic complexity class,

```{r atimeGradRef}
a_refs <- atime::references_best(a_res)
plot(a_refs)
```

The plot above suggests that memory usage (kilobytes) is linear (N),
and computation time (seconds) is linear or log-linear (N log N),
which is expected.

## Conclusions

We have shown how to compute our proposed AUM using R torch, and
compared the computation time of torch auto-grad with "explicit
gradients" from R package `aum`. We observed that there is some
overhead to using torch, but asymptotically the two methods take the
same amount of time: log-linear `O(N log N)`, in number of samples `N`.

## Session info

```{r}
sessionInfo()
```
