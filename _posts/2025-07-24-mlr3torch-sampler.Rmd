---
layout: post
title: A custom DataLoader for mlr3torch
description: Stratified sampling for imbalanced classification
---

```{r Ropts, echo=FALSE, results='hide'}
repo.dir <- normalizePath("..")
post.id <- "2025-07-24-mlr3torch-sampler"
fig.path <- paste0(file.path(repo.dir, "assets", "img", post.id), "/")
dir.create(fig.path, showWarnings = FALSE, recursive = TRUE)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=fig.path,
  fig.width=10, ## TODO python figures wider? look at prev issue.
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=4)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
options(width=120)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

The goal of this post is to show how to use a custom torch sampler with mlr3torch, in order to use stratified sampling, which can ensure that each batch in gradient descent has a minimum number of samples from each class.

## Motivation: imbalanced classification

We consider imbalanced classification problems, which occur frequently in many different areas.
For example, in a recent project involving predicting childhood autism, we used data from the National Survey of Children's Health (NSCH), which had about 3% autism, and 20K rows.

```{r}
library(data.table)
prop.pos <- 0.03
Nrow <- 20000
(aut_sim <- data.table(autism=rep(c(1,0), c(prop.pos, 1-prop.pos)*Nrow)))
```

To learn with these data in torch, we can use stochastic gradient descent.
To do that, we need to wrap the data table in a `dataset` as below:

```{r}
ds_gen <- torch::dataset(
  initialize=function(){},
  .getbatch=function(i)aut_sim[i]$autism,
  .length=function()nrow(aut_sim))
```

After that, we need to attach the `dataset` to a `dataloader` with a
certain batch size, which we define as 100 below:

```{r}
ds <- ds_gen()
batch_size <- 100
dl <- torch::dataloader(ds, batch_size=batch_size, shuffle=TRUE)
```

To iterate through the `dataloader` we use a loop, and we count the
number of each class in each batch:

```{r}
torch::torch_manual_seed(1)
count_dt_list <- list()
coro::loop(for (batch_tensor in dl) {
  batch_vec <- torch::as_array(batch_tensor)
  batch_id <- length(count_dt_list) + 1L
  count_dt_list[[batch_id]] <- data.table(
    batch_id,
    num_0=sum(batch_vec==0),
    num_1=sum(batch_vec==1))
})
(count_dt <- rbindlist(count_dt_list))
count_dt[num_1==0]
```

We can see above that there are `r nrow(count_dt)` batches that have no positive labels.
On average there should be 3 positive labels per batch, and in fact that is true:

```{r}
quantile(count_dt$num_1)
```

Above we see that the 50% quantile (median) of the number of positive labels per batch is equal to 3, as expected.

With typical loss functions, such as the cross-entropy (logistic) loss, you can still do gradient descent learning with a batch of all negative examples.
However that is not the case with complex loss functions like Area Under the Minimum (AUM) of False Positives and False Negatives, which we recently proposed for ROC curve optimization, in [Journal of Machine Learning Research 2023](https://jmlr.org/papers/v24/21-0751.html).
Computing the AUM requires computing a ROC curve, which is not possible without at least one positive and one negative example.
So in gradient descent learning with a batch of all negative examples, we can not compute the AUM, and we must skip it (waste of time).
It would be better if we could use stratified sampling, to control the number of minority class samples per batch.

## Stratified sampling

One way to implement stratified sampling is via the code below.
First we define a minimum number of samples per stratum:

```{r}
(min_samples_per_stratum <- prop.pos*batch_size)
```

The output above shows that there will be at least `r min_samples_per_stratum` samples from each stratum in each batch.
Below we shuffle the data set, and count the number of samples in each stratum:

```{r}
stratum <- "autism"
set.seed(1)
(shuffle_dt <- aut_sim[
, row.id := 1:.N
][sample(.N)][
, i.in.stratum := 1:.N, keyby=stratum
][])
```

The output above shows two new columns:

* `row.id` is the row number in the original data table.
* `i.in.stratum` is the row number of the shuffled data, relative to the stratum (autism).

Next, we count the number of samples per stratum.

```{r}
(count_dt <- shuffle_dt[, .(max.i=max(i.in.stratum)), by=stratum][order(max.i)])
(count_min <- count_dt$max.i[1])
```

Above, we see the smallest stratum has `r count_min` samples.
Next, we add a column `n.samp` with values between 0 and `r count_min`:

```{r}
shuffle_dt[
, n.samp := i.in.stratum/max(i.in.stratum)*count_min, by=stratum
][]
```

The idea is that `n.samp` can be used to control the number of samples we take from the smallest stratum.
If `n.samp <= 1`, then we take 1 sample from the smallest stratum, with a number of samples from other strata that is proportional.
In other words, we can use `n.samp` to define `batch.i`, a batch number:

```{r}
shuffle_dt[
, batch.i := ceiling(n.samp/min_samples_per_stratum)
][]
```

We see from the output above that `batch.i` is an integer from 1 to `r max(shuffle_dt$batch.i)`, that indicates in which batch each sample appears.
Below we see counts of each batch and class label.

```{r}
dcast(shuffle_dt, batch.i ~ autism, length)
```

The table above has one row per batch, and one column per class label.
We see that the class counts are constant across batches, consistent with stratified random sampling.

## Custom sampler

How to use the code above with `torch`?
We need to define a sampler class, as in the code below:

```{r}
hack_sampler_class <- torch::sampler(
  "HackSampler",
  initialize = function(data_source) {
    self$data_source <- data_source
  },
  .iter_batch = function(batch_size) {
    shuffle_dt <- aut_sim[
    , row.id := 1:.N
    ][sample(.N)][
    , i.in.stratum := 1:.N, keyby=stratum
    ][]
    count_dt <- shuffle_dt[, .(max.i=max(i.in.stratum)), by=stratum][order(max.i)]
    count_min <- count_dt$max.i[1]
    shuffle_dt[
    , n.samp := i.in.stratum/max(i.in.stratum)*count_min, by=stratum
    ][
    , batch.i := ceiling(n.samp/min_samples_per_stratum)
    ][]
    batch_list <- split(shuffle_dt, shuffle_dt$batch.i)
    count <- 0
    function() {
      if (count < length(batch_list)) {
        count <<- count + 1L
        return(batch_list[[count]]$row.id)
      }
      coro::exhausted()
    }
  },
  .length = function() {
    length(self$data_source)
  }
)
```

I call the code above a "hack" because it takes the same fixed value
for `min_samples_per_stratum` as defined in a previous code block (TODO: make this a parameter).
To use the sampler class, we must first instantiate it with a data set:

```{r}
hack_sampler_instance <- hack_sampler_class(ds)
```

Then we specify that instance as the sampler argument of the dataloader:

```{r}
hack_dl <- torch::dataloader(ds, sampler = hack_sampler_instance)
```

Finally we can loop over batches, to verify that the stratified sampling works.

```{r}
torch::torch_manual_seed(1)
count_dt_list <- list()
coro::loop(for (batch_tensor in hack_dl) {
  batch_vec <- torch::as_array(batch_tensor)
  batch_id <- length(count_dt_list) + 1L
  count_dt_list[[batch_id]] <- data.table(
    batch_id,
    num_0=sum(batch_vec==0),
    num_1=sum(batch_vec==1))
})
(count_dt <- rbindlist(count_dt_list))
```

## Plugging into mlr3torch

First, note that to get the code below to work, I needed to propose a modification to mlr3torch, which is available in this branch -- [PR](https://github.com/mlr-org/mlr3torch/pull/419) was merged, so should be on main.
Maybe need [batch_size](https://github.com/mlr-org/mlr3torch/pull/425) check hack to avoid an error?

Code to try:

* [batch_sampler](https://github.com/mlr-org/mlr3torch/issues/420#issue-3272006602)
* [sampler](https://github.com/mlr-org/mlr3torch/issues/420#issue-3272006602)

We can create a simple linear model torch learner in the mlr3 system via

```{r}
sonar_task <- mlr3::tsk("sonar")
sonar_task$col_roles$stratum <- "Class"
measure_list <- mlr3::msrs(c("classif.auc", "classif.logloss"))

mlp_learner <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
mlp_learner$predict_type <- "prob"
library(data.table)
stratified_sampler_class <- torch::sampler(
  "StratifiedSampler",
  initialize = function(data_source) {
    self$data_source <- data_source
    TSK <- data_source$task
    self$stratum <- TSK$col_roles$stratum
    self$stratum_dt <- data.table(
      TSK$data(cols=self$stratum),
      row.id=1:TSK$nrow)
  },
  .iter = function() {
    ## batch_size is actually min_samples_per_stratum, the number of
    ## samples from the smallest stratum in each batch.
    batch_size <- 10
    shuffle_dt <- self$stratum_dt[sample(.N)][
    , i.in.stratum := 1:.N, by=c(self$stratum)
    ][]
    count_dt <- shuffle_dt[, .(
      max.i=max(i.in.stratum)
    ), by=c(self$stratum)][order(max.i)]
    count_min <- count_dt$max.i[1]
    shuffle_dt[
    , n.samp := i.in.stratum/max(i.in.stratum)*count_min
    , by=c(self$stratum)
    ][
    , batch.i := ceiling(n.samp/batch_size)
    ][]
    batch_list <- split(shuffle_dt$row.id, shuffle_dt$batch.i)
    count <- 0
    function() {
      if (count < length(batch_list)) {
        count <<- count + 1L
        indices <- batch_list[[count]]
        print(indices)
        return(indices)
      }
      coro::exhausted()
    }
  },
  .length = function() {
    length(self$data_source)
  }
)
mlp_learner$param_set$set_values(
  epochs=10,
  ##batch_size=20,
  p=0,
  sampler=stratified_sampler_class,
  measures_valid=measure_list,
  measures_train=measure_list)
mlr3::set_validate(mlp_learner, 0.5)
mlp_learner$callbacks <- mlr3torch::t_clbk("history")
mlp_learner$train(sonar_task)

## why does batch_sampler not work?
measure_list <- mlr3::msrs(c("classif.auc", "classif.logloss"))
library(data.table)
stratified_sampler_class <- torch::sampler(
  "StratifiedSampler",
  initialize = function(data_source) {
    self$data_source <- data_source
    TSK <- data_source$task
    self$stratum <- TSK$col_roles$stratum
    self$stratum_dt <- data.table(
      TSK$data(cols=self$stratum),
      row.id=1:TSK$nrow)
  },
  .iter = function() {
    ## batch_size is actually min_samples_per_stratum, the number of
    ## samples frmo the smallest stratum in each batch.
    batch_size <- 10
    shuffle_dt <- self$stratum_dt[sample(.N)][
    , i.in.stratum := 1:.N, by=c(self$stratum)
    ][]
    count_dt <- shuffle_dt[, .(
      max.i=max(i.in.stratum)
    ), by=c(self$stratum)][order(max.i)]
    count_min <- count_dt$max.i[1]
    shuffle_dt[
    , n.samp := i.in.stratum/max(i.in.stratum)*count_min
    , by=c(self$stratum)
    ][
    , batch.i := ceiling(n.samp/batch_size)
    ][]
    batch_list <- split(shuffle_dt$row.id, shuffle_dt$batch.i)
    count <- 0
    function() {
      if (count < length(batch_list)) {
        count <<- count + 1L
        indices <- batch_list[[count]]
        print(indices)
        return(indices)
      }
      coro::exhausted()
    }
  },
  .length = function() {
    length(self$data_source)
  }
)
batch_learner <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
batch_learner$predict_type <- "prob"
batch_learner$param_set$set_values(
  epochs=10,
  batch_size=20,
  p=0,
  batch_sampler=stratified_sampler_class,
  measures_valid=measure_list,
  measures_train=measure_list)
mlr3::set_validate(batch_learner, 0.5)
batch_learner$callbacks <- mlr3torch::t_clbk("history")
batch_learner$train(sonar_task)

## positive control
rev_sampler_class <- torch::sampler(
  "MySampler",
  initialize = function(data_source) {
    print('init')
    self$data_source <- data_source
  },
  .iter = function() {
    count <<- 0L
    function() {
      if (count < length(self$data_source)) {
        idx <- length(self$data_source)-count
        count <<- count + 1L
        return(idx)
      }
      coro::exhausted()
    }
  },
  .length = function() {
    length(self$data_source)
  }
)
rev_learner <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
rev_learner$predict_type <- "prob"
rev_learner$param_set$set_values(
  epochs=10,
  batch_size=20,
  p=0,
  sampler=rev_sampler_class,
  measures_valid=measure_list,
  measures_train=measure_list)
mlr3::set_validate(rev_learner, 0.5)
rev_learner$callbacks <- mlr3torch::t_clbk("history")
rev_learner$train(sonar_task)


mlp_learner$model$callbacks$history
stratified_sampler_instance <- stratified_sampler_class(ds)
stratified_learner <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
stratified_learner$predict_type <- "prob"
measure_list <- mlr3::msrs(c("classif.auc", "classif.logloss"))
stratified_learner$param_set$set_values(
  epochs=10,
  batch_size=20,
  p=0,
  sampler=stratified_sampler_instance,
  measures_valid=measure_list,
  measures_train=measure_list)
mlr3::set_validate(stratified_learner, 0.5)
stratified_learner$callbacks <- mlr3torch::t_clbk("history")
stratified_learner$train(sonar_task)
stratified_learner$model$callbacks$history


# 1. Define a simple dataset
my_dataset <- torch::dataset(
  name = "MyDataset",
  initialize = function() {
    self$data <- torch::torch_tensor(1:5)
  },
  .getitem = function(index) {
    self$data[index]
  },
  .length = function() {
    length(self$data)
  }
)
# 2. Define a custom batch sampler 
batch_sampler_class <- torch::sampler(
  "BatchSampler",
  initialize = function(data_source) {
    self$data_source <- data_source
  },
  .iter = function() {
    batch_size <- 2
    indices <- 1:self$.length()
    batch_vec <- (indices-1) %/% batch_size
    batch_list <- rev(split(indices, batch_vec))
    count <- 0L
    function() {
      if (count < length(batch_list)) {
        count <<- count + 1L
        return(batch_list[[count]])
      }
      coro::exhausted()
    }
  },
  .length = function() {
    length(self$data_source)
  }
)
# 3. Instantiate dataset and sampler
ds <- my_dataset()
batch_sampler_instance <- batch_sampler_class(ds)
# 4. Create dataloader with custom sampler
dl <- torch::dataloader(ds, batch_sampler = batch_sampler_instance)
# 5. Iterate through the dataloader
batches <- list()
coro::loop(for (batch in dl) {
  batches[[length(batches) + 1L]] = batch
})
batches

sonar_task <- mlr3::tsk("sonar")
sonar_ingress <- list(feat=mlr3torch::TorchIngressToken(
  features=sonar_task$col_roles$feature,
  batchgetter=mlr3torch::batchgetter_num))
sonar_dataset <- mlr3torch::task_dataset(sonar_task, sonar_ingress)
inst_learner <- mlr3torch::LearnerTorchMLP$new(task_type="classif")
inst_learner$param_set$set_values(
  epochs=10,
  batch_size=20,
  batch_sampler=batch_sampler_class)
inst_learner$train(sonar_task)
```

```
> batch_learner$train(sonar_task)
 [1]  85  78  67  56  13 100  40  30  87   9
 [1] 39 55 25 51 24 19 96 18 12 15
 [1] 98 47 28 46 63 29 36 62 58 86
 [1] 61 88 64 17 52  5  7 34 59 77
 [1]  94  65  70  69  83  31   4 104  99   3
 [1] 97 80 71  2 33 27 10  8 43 41
 [1]  89  22  45  21   1  48 103  75  23  92
 [1]  81   6  53  82  16 101  38  91  60  73
 [1] 102  37  79  54  50  84  20  49  44  11
 [1] 35 32 66 26 74 72 42 76 90 95
[1] 93 57 14 68
Error in ctx$batch$y$to(device = ctx$device) : 
  attempt to apply non-function
> traceback()
15: train_loop(ctx, callbacks)
14: learner_torch_train(self, private, super, task, param_vals)
13: force(expr)
12: with_torch_settings(seed = param_vals$seed, num_threads = param_vals$num_threads, 
        num_interop_threads = param_vals$num_interop_threads, expr = {
            learner_torch_train(self, private, super, task, param_vals)
        })
11: .__LearnerTorch__.train(self = self, private = private, super = super, 
        task = task)
10: get_private(learner)$.train(task)
9: .f(learner = <environment>, task = <environment>)
8: eval(expr, p)
7: eval(expr, p)
6: eval.parent(expr, n = 1L)
5: invoke(.f, .args = .args, .opts = .opts, .seed = .seed, .timeout = .timeout)
4: encapsulate(learner$encapsulation["train"], .f = train_wrapper, 
       .args = list(learner = learner, task = task), .pkgs = learner$packages, 
       .seed = NA_integer_, .timeout = learner$timeout["train"])
3: learner_train(learner, task, train_row_ids = train_row_ids, mode = mode)
2: .__Learner__train(self = self, private = private, super = super, 
       task = task, row_ids = row_ids)
1: batch_learner$train(sonar_task)
```

## Conclusions

TODO

## Session info

```{r}
sessionInfo()
```
