---
layout: post
title: Optimal partitioning in R
description: Vectorized implementations
---

```{r Ropts, echo=FALSE, results='hide'}
repo.dir <- normalizePath("..")
post.id <- "2024-11-07-OPART-data-table"
fig.path <- paste0(file.path(repo.dir, "assets", "img", post.id), "/")
dir.create(fig.path, showWarnings = FALSE, recursive = TRUE)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=fig.path,
  fig.width=10, ## TODO python figures wider? look at prev issue.
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=4)
conda.env <- "2023-08-deep-learning"
conda.env <- "torch-aum"
RETICULATE_PYTHON <- sprintf(if(.Platform$OS.type=="unix")
  ##"/home/tdhock/.local/share/r-miniconda/envs/%s/bin/python"
  "/home/tdhock/miniconda3/envs/%s/bin/python"
  else "~/AppData/Local/Miniconda3/envs/%s/python.exe", conda.env)
Sys.setenv(RETICULATE_PYTHON=RETICULATE_PYTHON)
##reticulate::use_condaenv(dirname(RETICULATE_PYTHON), required=TRUE)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
options(width=120)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

The goal of this post is to show how to implement the optimal
partitioning algorithm in R.

# Simulate data sequence

The optimal partitioning algorithm (Jackson et al, 2005) is a classic
dynamic programming algorithm for change-point detection in sequential
data (measured over time or space). To illustrate its computation, we
simulate some data below.

```{r}
set.seed(1)
N.segs <- 3
N.dim <- 2
(mean.mat <- matrix(
  runif(N.segs*N.dim, 0, 10),
  N.segs, N.dim))
```

The code above creates a matrix with one column for each dimension of
data, and one row for each segment. The code below uses those segment
mean values to simulate a certain number of data points per segment,
using a normal distribution.

```{r}
library(data.table)
N.per.seg <- 1000
set.seed(1)
sim.dt.list <- list()
true.mean.dt.list <- list()
for(seg.i in 1:N.segs){
  for(dim.i in 1:N.dim){
    mean.param <- mean.mat[seg.i, dim.i]
    num.vec <- rnorm(N.per.seg, mean.param)
    true.mean.dt.list[[paste(seg.i, dim.i)]] <- data.table(
      seg.i, dim.i, mean.param,
      first.i=(seg.i-1)*N.per.seg+1, 
      last.i=seg.i*N.per.seg, 
      parameter="true"
    )[, `:=`(
      start=first.i-0.5,
      end=last.i+0.5
    )]
    sim.dt.list[[paste(seg.i, dim.i)]] <- data.table(
      seg.i, dim.i, i.in.seg=1:N.per.seg, value=num.vec)
  }
}
(true.mean.dt <- rbindlist(true.mean.dt.list))
```

One of the results of the code above is the data table of true means,
with one row for each segment and each dimension in the true signal.
The other result of the simulation is the table of simulated data,
which are shown below,

```{r}
(sim.dt <- rbindlist(
  sim.dt.list
)[
, data.i := seq_along(seg.i)
, by=dim.i][])
```

To visualize the simulated data, we use the code below.

```{r simulated-data}
library(ggplot2)
ggplot()+
  facet_grid(dim.i ~ ., labeller=label_both)+
  geom_point(aes(
    data.i, value),
    shape=1,
    data=sim.dt)+
  geom_segment(aes(
    start, mean.param,
    color=parameter,
    xend=end, yend=mean.param),
    size=2,
    data=true.mean.dt)+
  scale_x_continuous(
    "Position in data sequence")+
  scale_y_continuous(
    "Data value")
```

The figure above shows the simulated data (black points) and segment
means (orange line segments). The primary goal of change-point
detection algorithms like optimal partitioning is to recover the true
segment means, using only the noisy simulated data.

Another way to view these data is in matrix form, with one row per
position in the data sequence, and one column per dimension:

```{r}
sim.wide <- dcast(sim.dt, data.i ~ dim.i, value.var="value")
sim.mat <- as.matrix(sim.wide[,-1])
head(sim.mat)
```

# Implementing optimal partitioning in R

The goal of the optimal partitioning algorithm is to compute the
segmentation with the best cost, given a data set, and a non-negative
penalty value. The cost is defined as the total squared error plus a
penalty for each change-point added. The main idea of the algorithm is
to compute the optimal cost recursively, so we need to initialize a
vector of optimal cost values which start as missing, but we will fill in later:

```{r}
N.data <- nrow(sim.mat)
best.cost.vec <- rep(NA_real_, N.data)
```

## First data point

We start at the first data point, which in this case is:

```{r}
sim.mat[1,]
```

In a normal model, the best means are the data themselves, which
result in a total squared error of zero, which is our value for the
best cost at data point 1:

```{r}
best.mean1 <- sim.mat[1,]
(best.cost1 <- sum((sim.mat[1,]-best.mean1)^2))
best.cost.vec[1] <- best.cost1
```

## Second data point, slow computation with mean

The second data point is

```{r}
sim.mat[2,]
```

Now, we have a choice. Do we put a change-point between the first and
second data points, or not? For the model with no change-point, we
compute the cost using the mean matrix below:

```{r}
(mean.from.1.to.2 <- matrix(
  colMeans(sim.mat[1:2,]),
  nrow=2, ncol=2, byrow=TRUE))
```

The cost is the total squared error:

```{r}
(cost.from.1.to.2.no.change <- sum((mean.from.1.to.2-sim.mat[1:2,])^2))
```

## Second data point, fast computation with cumsum trick

It is difficult to see for the case of only two data points, but the
cost computation above is asymptotically slow, because it is linear in
the number of data points. We can reduce that to a constant time
operation, if we use the cumulative sum trick, with the cost
factorization trick.  Above we wrote the total squared error as the
sum of the squared difference, but using the factorization trick, we
can compute it using:

```{r}
(factorization.mat <- mean.from.1.to.2^2 - 2*mean.from.1.to.2*sim.mat[1:2,] + sim.mat[1:2,]^2)
sum(factorization.mat)
```

The calculation above is still O(N), linear time in the number of
data, because of the sum over rows. To get that down to constant time, we need
to first compute matrices of cumulative sums:

```{r}
cum.data <- rbind(0,apply(sim.mat,2,cumsum))
cum.squares <- rbind(0,apply(sim.mat^2,2,cumsum))
```

The code above is O(N) linear time, but it only needs to be computed once
in the entire algorithm. Below we use those cumulative sum matrices to
compute the cost in constant time.

```{r}
sum_trick <- function(m, start, end)m[end+1,,drop=FALSE]-m[start,,drop=FALSE]
cost_trick <- function(start, end){
  sum_trick(cum.squares, start, end)-
    sum_trick(cum.data, start, end)^2/
    (end+1-start)
}
(cost.1.2.trick.vec <- cost_trick(1,2))
sum(cost.1.2.trick.vec)
```

Actually the sum above is O(D), linear in the number of
columns/dimensions of data, but constant/independent of N, the number
of rows/observations of data (which is the important part for OPART to
be fast).

## atime comparison

We can verify the constant versus linear time complexity of the two computations using the atime code below:

```{r}
trick_vs_mean <- atime::atime(
  N=unique(as.integer(10^seq(1, 3, by=0.2))),
  trick=sum(cost_trick(1,N)),
  mean={
    sim.N.mat <- sim.mat[1:N,]
    mean.mat <- matrix(
      colMeans(sim.N.mat),
      nrow=N, ncol=2, byrow=TRUE)
    sum((sim.N.mat-mean.mat)^2)
  },
  result=TRUE)
plot(trick_vs_mean)
```

It is clear from the plot above that time and memory usage increase
with N for the mean method, but not for the trick method. The code
below shows that they get the same result:

```{r}
(trick_vs_mean_result <- dcast(
  trick_vs_mean$measurements,
  N ~ expr.name,
  value.var="result"))
trick_vs_mean_result[, all.equal(mean, trick)]
```

## Second data point, cost of change-point

We also have to consider the model with a change-point. In that case,
there are two segments with zero squared error, and so the cost is
equal to the penalty. Let's take a penalty value of

```{r}
penalty <- 15
```

So the two options are:
```{r}
cost.from.1.to.2.no.change
(cost.from.1.to.2.change <- penalty)
```

We combine them into a vector:
```{r}
(cost.from.1.to.2.candidates <- c(cost.from.1.to.2.no.change, cost.from.1.to.2.change))
```

We then find the min cost in that vector:

```{r}
(best.i.from.1.to.2 <- which.min(cost.from.1.to.2.candidates))
```

And we end this iteration (for data point 2) by saving these values
for the next iteration:

```{r}
best.change.vec <- rep(NA_integer_, N.data)
best.change.vec[2] <- best.i.from.1.to.2
best.cost.vec[2] <- cost.from.1.to.2.candidates[best.i.from.1.to.2]
data.table(best.cost.vec,best.change.vec)
```

## Third data point

The third data point is where we can start to see how to write a
general and efficient implementation of the optimal partioning
algorithm. We want to compute the best model among these candidates:

* last segment starts at data point 1: no changes.
* last segment starts at data point 2: change between data points 1 and 2.
* last segment starts at data point 3: change between data points 2
  and 3, and maybe another change before that.
  
Actually, for the third candidate, we already know that the best model
up to data point 2 does not include a change between 1 and 2, and
exploiting that existing result is the main idea of the dynamic
programming algorithm.
To compute the cost of these three candidates, we first compute the cost of the last segment in each candidate:

```{r}
up.to <- 3
(last.seg.cost <- rowSums(cost_trick(1:up.to, rep(up.to, up.to))))
```

We also need to compute the cost of the other segments, before the last one. 

* For the case of the first candidate, there are no other segments, so the cost is zero. 
* For the case of the other candidates, we can use the optimal cost which has already been computed (this is dynamic programming), but we need to add the penalty for an additional change-point.

```{r}
(other.seg.cost <- c(0, best.cost.vec[seq(1, up.to-1)]+penalty))
```

The total cost is the sum of the cost on the last segment, and the other segments:

```{r}
(total.seg.cost <- last.seg.cost+other.seg.cost)
```

The iteration ends with a minimization:

```{r}
(best.i.from.1.to.3 <- which.min(total.seg.cost))
best.change.vec[3] <- best.i.from.1.to.3
best.cost.vec[3] <- total.seg.cost[best.i.from.1.to.3]
data.table(best.cost.vec,best.change.vec)
```

## Other data points

More generally, we can use the for loop below to compute the optimal
cost up to any data point,

```{r}
for(up.to in 1:N.data){
  last.seg.cost <- rowSums(cost_trick(1:up.to, rep(up.to, up.to)))
  change.cost <- if(up.to>1)best.cost.vec[seq(1, up.to-1)]+penalty
  other.seg.cost <- c(0, change.cost)
  total.cost <- other.seg.cost+last.seg.cost
  best.i <- which.min(total.cost)
  best.change.vec[up.to] <- best.i
  best.cost.vec[up.to] <- total.cost[best.i]
}
data.table(change=best.change.vec, cost=best.cost.vec)
```

The table above shows the first/last few rows of the resulting optimal
cost and change. 

# Decoding in R

How can we use the table of optimal cost and change indices to find
the optimal segmentation? The last row of the optimal cost/change
table tells us where to look for the previous change-point:

```{r}
(last.seg.start <- best.change.vec[N.data])
```

We see that the last segment starts at the data point given in the
result above. We can look at the entry just before that
to determine the previous change-point:

```{r}
(second.to.last.seg.start <- best.change.vec[last.seg.start-1])
```

We see that the second to last segment starts at the data point given
in the result above. We can look at the entry just before that to
determine the previous change-point:

```{r}
(third.to.last.seg.start <- best.change.vec[second.to.last.seg.start-1])
```

We see the result above is 1, the first data point, so this must be
the first segment. This algorithm is commonly known as "decoding" the
optimal change position vector, to determine the optimal segmentation.
Translating the logic above to a for loop, we get the code below:

```{r}
seg.dt.list <- list()
last.i <- length(best.change.vec)
while(print(last.i)>0){
  first.i <- best.change.vec[last.i]
  seg.dt.list[[paste(last.i)]] <- data.table(
    first.i, last.i,
    mean=sum_trick(cum.data, first.i, last.i)/(last.i+1-first.i))
  last.i <- first.i-1L
}
rbindlist(seg.dt.list)[seq(.N,1)]
```

Compare the result above from dynamic programming to the true values
from the simulation below:

```{r}
mean.mat
```

It is clear that dynamic programming computes a segmentation model
with mean values that closely match the true values from the
simulation.

## Pruning

The Pruned Exact Linear Time (PELT) algorithm of Killick et al. (2012)
allows us to prune the set of change-points, while retaining the same
optimal solution. To implement that algorithm, we again allocate
vectors to store the optimal cost and change-points.

```{r}
pelt.change.vec <- rep(NA_integer_, N.data)
pelt.cost.vec <- rep(NA_real_, N.data+1)
pelt.cost.vec[1] <- -penalty
```

Above we initialize the first element of the cost vector to the
negative penalty, so we don't have to treat the model with no changes
as a special case. We can view `pelt.cost.vec[i]` as the best cost up
to but not including data point `i` (and unlike the previous
formulation, we always have to add a penalty to obtain the overall
cost). Below we initialize vectors to store the total number of
candidates considered by the algorithm, as well as the min and max
over all candidates considered:

```{r}
pelt.candidates.vec <- rep(NA_integer_, N.data)
pelt.candidates.min <- rep(NA_integer_, N.data)
pelt.candidates.max <- rep(NA_integer_, N.data)
```

We initialize the vector of candidates, then proceed in a loop over data points.

```{r}
candidate.vec <- 1L
for(up.to in 1:N.data){
  N.cand <- length(candidate.vec)
  pelt.candidates.vec[up.to] <- N.cand
  pelt.candidates.min[up.to] <- min(candidate.vec)
  pelt.candidates.max[up.to] <- max(candidate.vec)
  last.seg.cost <- rowSums(cost_trick(candidate.vec, rep(up.to, N.cand)))
  prev.cost <- pelt.cost.vec[candidate.vec]
  cost.no.penalty <- prev.cost+last.seg.cost
  total.cost <- cost.no.penalty+penalty
  best.i <- which.min(total.cost)
  pelt.change.vec[up.to] <- candidate.vec[best.i]
  total.cost.best <- total.cost[best.i]
  pelt.cost.vec[up.to+1] <- total.cost.best
  candidate.vec <- c(candidate.vec[cost.no.penalty < total.cost.best], up.to+1L)
}
```

The pruning rule is implemented in the last line of the loop above. We
keep only the candidates that have a cost without penalty which is
less than the total cost of the best model. We can view the results of
the algorithm in the table below:

```{r}
(pelt.dt <- data.table(
  iteration=seq_along(pelt.change.vec),
  change=pelt.change.vec,
  cost=pelt.cost.vec[-1],
  candidates=pelt.candidates.vec,
  min.candidate=pelt.candidates.min,
  max.candidate=pelt.candidates.max))
```

Above the "candidate" columns let us analyze the empirical time
complexity of the algorithm (more candidates to consider mean a slower
algorithm). Below we verify that the `change` and `cost` columns are
consistent with the values we obtained using OPART.

```{r}
all.equal(pelt.dt$cost, best.cost.vec)
all.equal(pelt.dt$change, pelt.change.vec)
```

Below we visualize the change-point candidates that are considered
during the two algorithms.

```{r candidatesPELT}
ggplot()+
  geom_point(aes(
    iteration, iteration, color=algorithm),
    size=4,
    data=data.table(pelt.dt, y="number of candidates", algorithm="OPART"))+
  geom_point(aes(
    iteration, candidates, color=algorithm),
    size=2,
    data=data.table(pelt.dt, y="number of candidates", algorithm="PELT"))+
  facet_grid(y ~ ., scales="free")+
  geom_segment(aes(
    iteration, 1,
    color=algorithm,
    xend=iteration, yend=iteration),
    data=data.table(pelt.dt, y="candidate range", algorithm="OPART"))+
  geom_segment(aes(
    iteration, min.candidate,
    color=algorithm,
    xend=iteration, yend=max.candidate),
    data=data.table(pelt.dt, y="candidate range", algorithm="PELT"))
```

Above we can see two panels:

* `candidate range` shows us the min/max of the indices considered for the minimization, in each iteration of the dynamic programming for loop. We see three small triangles along the diagonal for OPART (indicating linear time), and a large triangle for PELT (indicating quadratic time).
* `number of candidates` shows us ho many indices are considered for the minimization, in each iteration of the dynamic programming for loop. We see that the number of candidates never exceeds 1000, because these data have a significant change every 1000 data points. In contrast, the number of candidates considered by OPART grows to the data size (3000).

The figure above clearly shows why the algorithm was named "Linear
Time" -- the number of candidates considered depends on the number of
data points per segment (not the overall number of data points).

# Converting to functions

```{r}
OPART <- function(sim.mat, penalty){
  cum.data <- rbind(0, apply(sim.mat, 2, cumsum))
  cum.squares <- rbind(0, apply(sim.mat^2, 2, cumsum))
  best.cost <- rep(NA_real_, nrow(sim.mat))
  best.change <- rep(NA_integer_, nrow(sim.mat))
  for(last.seg.end in seq_along(best.cost)){
    last.seg.start <- seq(1, last.seg.end)
    get_sum <- function(m){
      matrix(
        m[last.seg.end+1,], last.seg.end, ncol(m), byrow=TRUE
      )-m[last.seg.start,]
    }
    N.vec <- last.seg.end-last.seg.start+1
    last.seg.cost <- rowSums(get_sum(cum.squares)-get_sum(cum.data)^2/N.vec)
    other.cost <- c(0,best.cost[last.seg.start-1]+penalty)
    total.cost <- other.cost+last.seg.cost
    best.i <- which.min(total.cost)
    best.change[last.seg.end] <- best.i
    best.cost[last.seg.end] <- total.cost[best.i]
  }
  data.table(best.change,best.cost)
}
obase <- OPART_fast(sim.mat,penalty)

suffix.vec <- c("dt","dt_update","fast")
ores <- atime::atime(
  N=unique(as.integer(10^seq(1,3,by=0.2))),
  setup={
    str(N)
    x.mat <- sim.mat[1:N,]
  },
  seconds.limit=0.1,
  expr.list=atime::atime_grid(
    list(FUN=paste0("OPART_",suffix.vec)),
    OPART=FUN(x.mat,15),
    symbol.params = "FUN"))
plot(ores)

oref <- atime::references_best(ores)
plot(oref)
all.equal(why.slow$best.cost, obase$best.cost)

decode <- function(best.change){
  seg.dt.list <- list()
  last.i <- length(best.change)
  while(print(last.i)>0){
    first.i <- best.change[last.i]
    seg.dt.list[[paste(last.i)]] <- data.table(
      first.i, last.i)
    last.i <- first.i-1L
  }
  rbindlist(seg.dt.list)[seq(.N,1)]
}
decode(obase$best.change)

min.list <- list(
  base=function(x){
    i <- which.min(x)
    x[i]
  },
  dt=function(x){
    data.table(x)[which.min(x), x]
  })
ares <- atime::atime(
  setup={
    set.seed(1)
    x.vec <- rnorm(N)
  },
  expr.list=atime::atime_grid(
    list(fun=names(min.list)),
    min=min.list[[fun]](x.vec)))
plot(ares)
```

## Conclusions

TODO

## Session info

```{r}
sessionInfo()
```
