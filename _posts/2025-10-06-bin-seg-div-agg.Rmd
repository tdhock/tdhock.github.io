---
layout: post
title: Agglomerative hierarhical binary segmentation
description: Clustering using loss or distance minimization
---

```{r Ropts, echo=FALSE, results='hide'}
repo.dir <- normalizePath("..")
post.id <- "2025-10-06-bin-seg-div-agg"
fig.path <- paste0(file.path(repo.dir, "assets", "img", post.id), "/")
dir.create(fig.path, showWarnings = FALSE, recursive = TRUE)
knitr::opts_chunk$set(
  dpi=100,
  fig.path=fig.path,
  fig.width=10, ## TODO python figures wider? look at prev issue.
  fig.process=function(path)sub(repo.dir, "", path, fixed=TRUE),
  fig.height=4)
in_render <- !is.null(knitr::opts_knit$get('rmarkdown.pandoc.to'))
in_knit <- isTRUE(getOption('knitr.in.progress'))
options(width=150)
if(FALSE){
  knitr::knit(paste0(post.id, ".Rmd"))
}
```

The goal of this post is to explain how to compute the classic binary segmentation heuristic algorithm for change-point detection.

# Example data

We begin by loading an example data set.

```{r}
data(neuroblastoma, package="neuroblastoma")
library(data.table)
nb.dt <- data.table(neuroblastoma[["profiles"]])
one.dt <- nb.dt[profile.id==4 & chromosome==2]
one.dt
```

The output above shows a table with a `logratio` column that we will use as input to binary segmentation.
We visualize those data below, as a function of row number in the table.

```{r neuro-data}
library(ggplot2)
one.dt[, data.i := .I]
ggplot()+
  scale_x_continuous(
    limits=c(0, nrow(one.dt)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=one.dt)
```

The figure above shows a black dot for each data point to segment.

# Binary segmentation

We will consider a parametric model: change in mean with constant variance.
Assuming the normal distribution, maximizing the likelihood is equivalent to minimizing the square loss.
For an efficient implementation of binary segmentation in this case, we can use the cumulative sum trick.

```{r}
(cum.dt <- one.dt[, data.table(
  data=c(0,cumsum(logratio)),
  square=c(0,cumsum(logratio^2)))])
```

The output above is a table of cumulative sums of the `data` and their `square`.
This table is useful because it allows constant time computation of the optimal square loss for any given segment, for example the segment (1,N) on all of the data:

```{r}
square_loss <- function(start, end)cum.dt[
, square[end+1]-square[start]-(data[end+1]-data[start])^2/(end+1-start)]
N_data <- nrow(one.dt)
(loss <- square_loss(1, N_data))
```

## Binary segmentation via splitting

For a segment with N data, we can compute the loss of each candidate split on that segment in linear `O(N)` time via

```{r}
get_diff <- function(start, end, change){
  loss_together <- square_loss(start, end)
  loss_apart <- square_loss(start, change)+square_loss(change+1, end)
  data.table(start, end, change, loss_diff=loss_together-loss_apart)
}
get_diff_for_seg <- function(start, end){
  get_diff(start, end, seq(start, end-1L))
}
(first_candidates <- get_diff_for_seg(1, N_data))
```

Each row in the table above represents a candidate change-point after splitting the first segment (1,N).
The `loss_diff` column can be maximized to obtain the best split point.

```{r}
first_candidates[which.max(loss_diff)]
```

The output above shows that the best first split is a change after data point 41.
The code below implements this update rule recursively:

```{r loss-split}
new.segs <- data.table(start=1L, end=N_data)
split.dt.list <- list()
loss <- square_loss(1, N_data)
cand.segs <- NULL
for(n_segs in seq(1, N_data)){
  new.loss <- new.segs[
  , get_diff_for_seg(start, end)[which.max(loss_diff)]
  , by=.I]
  cand.segs <- rbind(cand.segs, new.loss)
  best.i <- which.max(cand.segs$loss_diff)
  best <- cand.segs[best.i]
  split.dt.list[[n_segs]] <- data.table(
    iteration=n_segs,
    segments=n_segs,
    n_in_max=nrow(cand.segs),
    computed=nrow(new.segs),
    best,
    loss)
  loss <- loss-best$loss_diff
  new.segs <- best[, rbind(
    data.table(start, end=change),
    data.table(start=change+1L, end))]
  cand.segs <- cand.segs[-best.i]
}
(split.dt <- rbindlist(split.dt.list))
plot(loss ~ segments, split.dt)
```

The output table above has one row per split in classic divisive binary segmentation, referred to as [Binary segmentation in section 5.2.2 of the Truong review paper](https://arxiv.org/pdf/1801.00718).
The figure represents the loss as a function of model size, and is often used for model selection using the slope heuristic (choose the model where the loss starts to become flat).
The R code above is sub-optimal in time and space complexity, because a new `cand.segs` table must be allocated in each iteration (worst case linear time).
The number of candidates considered changes with the number of segments, as can be seen below.

```{r candidates-split}
split.long <- melt(split.dt, measure.vars=c("n_in_max", "computed"))
ggplot()+
  geom_point(aes(
    iteration, value),
    data=split.long)+
  facet_grid(variable ~ ., scales="free")
```

The output above shows that the number of candidates grows to about 50
in the first 100 segments.  So the algorithm is quadratic time, and
this could be fixed by moving the code to C++, as in the
[binsegRcpp](https://cloud.r-project.org/web/packages/binsegRcpp/)
package, which uses special STL containers (`multiset`,
`priority_queue`) to achieve best case log-linear time complexity.

## Agglomerative binary segmentation (bottom-up)

Another way to compute change-points is by starting with N separate clusters/segments, and performing N-1 join operations, until we obtain a single cluster/segment.
Like the classic divisive binary segmentation described in the previous section, this is a hierarchical segmentation method (result is a tree), and it is referred to as [Bottom-up segmentation in section 5.2.3 of the Truong review paper](https://arxiv.org/pdf/1801.00718).
It is implemented below.

```{r loss-join}
join.dt.list <- list()
join.edge.list <- list()
iteration <- 1
cluster.dt <- data.table(start=1:N_data, end=1:N_data, loss_diff=NA_real_)
while(nrow(cluster.dt)>1){
  todo <- cluster.dt[-.N, which(is.na(loss_diff))]
  new.edges <- cluster.dt[, get_diff(start[todo], end[todo+1], end[todo])]
  cluster.dt[todo, loss_diff := new.edges$loss_diff]
  join.edge.list[[paste(nrow(cluster.dt))]] <- data.table(
    iteration,
    cluster.dt[-.N])
  best.i <- which.min(cluster.dt$loss_diff)
  join.dt.list[[paste(nrow(cluster.dt))]] <- data.table(
    iteration,
    segments=nrow(cluster.dt),
    n_in_min=nrow(cluster.dt),
    computed=length(todo),
    cluster.dt[best.i])
  new.cluster <- cluster.dt[, data.table(
    start=start[best.i],
    end=end[best.i+1],
    loss_diff=NA_real_)]
  cluster.dt[best.i-1, loss_diff := NA_real_]
  others <- cluster.dt[-c(best.i,best.i+1)]
  cluster.dt <- rbind(new.cluster, others)
  setkey(cluster.dt, start)
  iteration <- iteration+1L
}
(join.dt <- rbindlist(join.dt.list)[, loss := cumsum(loss_diff)][])
plot(loss ~ segments, join.dt)
```

The output above is another table, where the `end` column represents the sequence of change-points.
The loss figure above is similar to the one in the previous section.
Below we plot the number of candidates as a function of number of segments.

```{r join-computed}
join.long <- melt(join.dt, measure.vars=c("n_in_min", "computed"))
ggplot()+
  geom_point(aes(
    iteration, value),
    data=join.long)+
  facet_grid(variable ~ ., scales="free")
```

We see a very different pattern in the figure above: the first iteration has the most items in the minimization, which decreases linearly.
Using a priority queue in C++ would definitely result in big speed improvements:

* each search for best join would be `O(log N)` instead of `O(N)`,
* overall the algorithm would be `O(N log N)` instead of `O(N^2)`.

## Comparing the change-points for small model sizes

Do the two algorithms yield the same results?
Sometimes, but not always, as we can see in the three examples below.


```{r changes-small-models}
both.dt <- rbind(
  join.dt[, data.table(algo="join", segments, loss, change=end)],
  split.dt[, data.table(algo="split", segments=segments+1, loss, change)])
show.segs <- 2:4
seg.dt <- data.table(Segments=show.segs)[, {
  both.dt[segments<=Segments, {
    schange <- sort(change)
    start <- c(1L, schange+1L)
    end <- c(schange, N_data)
    total <- cum.dt[, data[end+1]-data[start] ]
    data.table(start, end, mean=total/(end+1-start))
  }, by=algo]
}, by=Segments]
loss.text <- both.dt[segments %in% show.segs][, Segments := segments]
model.color <- "blue"
ggplot()+
  scale_x_continuous(
    limits=c(0, nrow(one.dt)+1))+
  scale_y_continuous(
    "logratio (noisy copy number measurement)")+
  geom_point(aes(
    data.i, logratio),
    data=one.dt)+
  geom_vline(aes(
    xintercept=start-0.5),
    data=seg.dt[start>1],
    linewidth=1,
    linetype="dashed",
    color=model.color)+
  geom_segment(aes(
    start-0.5, mean,
    xend=end+0.5, yend=mean),
    data=seg.dt,
    linewidth=2,
    color=model.color)+
  geom_text(aes(
    230, -0.5, label=sprintf("loss=%.1f", loss)),
    data=loss.text,
    hjust=1,
    color=model.color)+
  facet_grid(Segments ~ algo, labeller=label_both)
```

The figure above shows the results for the two algorithms (panels from left to right), and three model sizes (panels from top to bottom).

* For two segments, `algo=split` has a smaller loss value.
* For three segments, `algo=join` has a smaller loss value.
* For four segments, both algorithms are the same.

## Comparing all loss values

Another way to compare is by examining the loss values for some small model sizes:

```{r loss-small-models}
show.max <- 19
ggplot()+
  theme_bw()+
  geom_point(aes(
    segments, loss, color=algo, size=algo),
    data=both.dt[segments<=show.max])+
  scale_size_manual(values=c(
    join=3,
    split=2))+
  scale_x_continuous(breaks=1:show.max)
```

Above we see that there are large loss differences for two small models (2 and 3 segments), and small differences for larger models.

```{r loss-all-models}
wide.dt <- dcast(both.dt, segments ~ algo, value.var="loss")[
, diff := join-split
][
  !is.na(diff)
][, let(better = fcase(
  abs(diff)<1e-9, "same",
  join<split, "join", 
  default="split"
))][]
ggplot()+
  geom_point(aes(
    segments, abs(diff), color=better),
    data=wide.dt)+
  scale_y_log10("Loss difference")+
  scale_color_discrete(breaks=c("split","join","same"))+
  theme(legend.position=c(0.5,0.5))
```

Above we see the absolute loss difference as a function of model size. We see that sometimes the two algorithms result in equally good loss values, and sometimes one is better than the other.

# Comparing with hierarchical clustering

Note that the algorithms implemented above are similar to another classic algorithm: [agglomerative/hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering), implemented as `hclust()` in R.
But these are not the same in general! See [my slides on hierarchical clustering](https://github.com/tdhock/2023-08-unsupervised-learning/blob/main/slides/04-hierarhical-clustering.pdf) for details about how it works.
Briefly, the hierarchical clustering algorithm starts with `N` clusters, one for each data point, and computes pairwise distances, represented in an `NxN` matrix.
It then searches for the min distance, which takes `O(N^2)` time, and joins the corresponding clusters.
Repeating this join operation `N` times results in the cluster tree, overall `O(N^3)` time in general.
There are various ways to speed this up: 

* using a priority queue to compute the min distance can reduce an `N` factor to `log N`.
* restricting the number of pairwise distances to a neighborhood graph can reduce the `N^2` number of distances to minimize over to `N` (for example on a 2d grid).

## Visualizing pairwise distance matrices

Below we code a simple (inefficient) method in R.
The first step is to compute the pairwise distance matrix, shown below.

```{r full-dist-mat}
data.mat <- as.matrix(one.dt[, c("logratio")])
data.mat.dt <- data.table(
  row=as.integer(row(data.mat)),
  col=as.integer(col(data.mat)),
  logratio=as.numeric(data.mat))
nb.dist <- dist(data.mat)
nb.dist.mat <- as.matrix(nb.dist)
nb.dist.dt <- data.table(
  iteration=0L,
  row=as.integer(row(nb.dist.mat)),
  col=as.integer(col(nb.dist.mat)),
  dist=as.numeric(nb.dist.mat))
ggplot()+
  geom_tile(aes(
    col, row, fill=dist),
    data=nb.dist.dt)+
  scale_fill_gradient(low="white", high="red")+
  coord_equal()+
  scale_y_reverse("observation")+
  scale_x_continuous("observation")
```

The figure above shows the pairwise distance matrix as a heatmap.
Since it is symmetric, we can keep only the lower triangle, without losing information.

```{r dist-lower}
lower.triangle <- nb.dist.dt[col<row]
ggplot()+
  geom_tile(aes(
    col, row, fill=dist),
    data=lower.triangle)+
  scale_fill_gradient(low="white", high="red")+
  coord_equal()+
  scale_y_reverse("observation")+
  scale_x_continuous("observation")
```

The figure above shows the lower triangle, representing distances that we could potentially use in the clustering algorithm.
The first iteration of the algorithm uses only the distances in the diagonal band shown below.

```{r dist-band}
diag.band <- nb.dist.dt[col+1==row]
ggplot()+
  geom_tile(aes(
    col, row, fill=dist),
    data=diag.band)+
  scale_fill_gradient(low="white", high="red")+
  coord_equal()+
  scale_y_reverse("observation")+
  scale_x_continuous("observation")
```

It is difficult to see the band which is one lower than the diagonal in the figure above.
Because there is only a diagonal band (1d join constraints), we get a linear time initialization (rather than quadratic as in the usual multivariate clustering algorithm).
Are these the same values as in agglomerative binary segmentation?

```{r compare-first-it}
join.edge <- rbindlist(join.edge.list)
compare.first.iteration <- data.table(join.edge[iteration==1, .(loss_diff)], diag.band)
ggplot()+
  geom_point(aes(
    loss_diff, dist),
    shape=1,
    data=compare.first.iteration)+
  scale_x_log10()+
  scale_y_log10()
```

The figure above shows that for the first iteration of this distance/linkage clustering algorithm, the distances on the Y axis are consistent with the loss difference values from the previous loss minimization binary segment joining algorithm.

## Clustering algorithm

Below we code the iterations of the clustering algorithm.
Some parts of the code below are similar to the previous algorithm above, but there is a new `dist_next` column (instead of the previous `loss_diff` column).
Distance updates use the single linkage criterion: distance between two clusters is defined as the min distance between points in clusters.

```{r}
agg.dt.list <- list()
agg.edge.list <- list()
iteration <- 1
cluster.dt <- data.table(start=1:N_data, end=1:N_data, dist_next=c(diag.band$dist, NA))
more.dist.list <- list()
while(nrow(cluster.dt)>1){
  agg.edge.list[[paste(nrow(cluster.dt))]] <- data.table(
    iteration,
    cluster.dt[-.N])
  best.i <- cluster.dt[-.N, which.min(dist_next)]
  agg.dt.list[[paste(nrow(cluster.dt))]] <- data.table(
    iteration,
    segments=nrow(cluster.dt),
    n_in_min=nrow(cluster.dt)-1,
    cluster.dt[best.i])
  if(best.i>1){
    bottom.cluster <- cluster.dt[best.i+1]
    bottom.indices <- with(bottom.cluster, start:end)
    bottom.to.prev <- nb.dist.mat[bottom.indices, best.i-1]
    more.dist.list[[paste(iteration, "bottom")]] <- data.table(
      iteration,
      row=bottom.indices,
      col=best.i-1L,
      dist=bottom.to.prev)      
    cluster.dt[best.i-1, dist_next := min(bottom.to.prev, dist_next)]
  }
  new.cluster <- cluster.dt[, data.table(
    start=start[best.i],
    end=end[best.i+1],
    dist_next=dist_next[best.i+1])]
  if(best.i+1 < nrow(cluster.dt)){
    top.cluster <- cluster.dt[best.i]
    top.indices <- with(top.cluster, start:end)
    top.to.next <- nb.dist.mat[top.indices, best.i+2]
    more.dist.list[[paste(iteration, "bottom")]] <- data.table(
      iteration,
      row=top.indices,
      col=best.i+2L,
      dist=top.to.next)
    new.cluster[, dist_next := min(dist_next, top.to.next)]
  }
  others <- cluster.dt[-c(best.i,best.i+1)]
  cluster.dt <- rbind(new.cluster, others)
  setkey(cluster.dt, start)
  iteration <- iteration+1L
}
(agg.dt <- rbindlist(agg.dt.list))
```

The result table above shows one row per iteration of the clustering algorithm.
Below we compare with iterations of the previous algorithm.

```{r}
data.table(agg.dt[, .(iteration, distance=end)], loss=join.dt$end)
```

Above we see a table with one row per iteration of the clustering algorithm.
The first two rows show that the same join events are chosen for the first two iterations, but in general they are not the same.
And in fact the last iterations of the algorithm are quite different (distance-based algo joins single data points near the start of the data sequence).

Below we visualize the distances involved in the algorithm.

```{r dist-used}
more.dist <- rbindlist(more.dist.list)
used.dist <- rbind(diag.band, more.dist)[, let(
  pmin = pmin(row,col),
  pmax = pmax(row,col))][]
ggplot()+
  geom_tile(aes(
    pmin, pmax, fill=dist),
    data=used.dist)+
  scale_fill_gradient(low="white", high="red")+
  coord_equal()+
  scale_y_reverse("observation")+
  scale_x_continuous("observation")
```

It can be seen that a substantial part of the lower triangle was actually used in the single linkage clustering algorithm.
Note this is more efficient than the typical multivariate hierarhical distance-based clustering, which uses the entire lower triangle.
However it is substantially less efficient than the loss-based binary segmentation, which would be log-linear with an efficient C++ implementation.

## Conclusions

We have explored different methods for segmentation of data in 1d, using either loss minimization segmentation (divisive or bottom up joining), or distance minimization clustering (joining).

## Session info

```{r}
sessionInfo()
```
